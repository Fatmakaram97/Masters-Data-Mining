{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "## Problem Formulation: ‚úç\n",
    "* We need to know the predicted probability of whether the new record (person) will match with another record (person) in the speed dating. \n",
    "\n",
    "### Input:\n",
    "* General features for the persons who attended the speed dating program collected, and the information collected from 4 different surveys applied, one survey before the start of the speed dating, two during the program, and one after finishing the speed dating. Some of them have the match value ( train dataset), and some don't have the match value (test dataset). Datasets contain many missing values which needs to be dealed with and organized.\n",
    "\n",
    "### Output:\n",
    "* We need to predict the probability of match value for the given dataset (test dataset). So, this would help to know if the person will match with another in the speed dating (has high probability to match) or not (has low probability to match).\n",
    "\n",
    "\n",
    "## What data mining function is required? üïµüèΩ\n",
    "* In this assignment we need \"classification and prediction\" data mining function.\n",
    "\n",
    "## What could be the challenges? üòï\n",
    "* Challenges are: how to make data organization and deal with the missing values and the unbalanced weights. How to tune the hyperparameters for each model and get the best hyperparameters for the model. In addition to how to develop a successful solution for the problem.\n",
    "\n",
    "## What is the impact? ü§ì\n",
    "* The impact is to know more about preprocessing for the data, and deal with the hyperparameters to see how they can affect the model and the results (predicted probability of match values), and how to deal with the pipelines. So, by making a good algorithm this would give the right predictions then this would help the organizers of the speed dating program to know the match value for each person before starting the program, so this would help more in knowing whether the person will be match with another or not. Then organizers of the program can know who will match in this time, so by those predictions the organizers will put each time people who have match value, so this will help in saving time in the program and the persons.\n",
    "\n",
    "## What is an ideal solution? ü¶∏\n",
    "* The ideal solution is to make very good learning for the model that will give perfect performance parameters, and this happens after making good preprocessing steps on the data. So, the model can predict the match value for each person to know whether he will find a suitable person to match with or not.\n",
    "\n",
    "## What is the experimental protocol used and how was it carried out? ü§î\n",
    "* I used the crossvalidation method in some trials, and the holdout method in some trials. So in the crossvalidation the algorithm splits the input data into training, and validation datasets, and in the holdout method I am working by splitting the given train dataset into two parts (train and test) and by selecting the test size I needed. I used the new parts for the train and test to tune the hyperparameters and get the know the best hyperparameters for the models I created by calculating the AUROC score on that test part which I took from the given training set so I have the right labels and by getting the predicted labels I could calculate the AUROC score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries üìù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read training, and test datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>idg</th>\n",
       "      <th>condtn</th>\n",
       "      <th>wave</th>\n",
       "      <th>round</th>\n",
       "      <th>position</th>\n",
       "      <th>positin1</th>\n",
       "      <th>order</th>\n",
       "      <th>partner</th>\n",
       "      <th>pid</th>\n",
       "      <th>...</th>\n",
       "      <th>attr3_3</th>\n",
       "      <th>sinc3_3</th>\n",
       "      <th>intel3_3</th>\n",
       "      <th>fun3_3</th>\n",
       "      <th>amb3_3</th>\n",
       "      <th>attr5_3</th>\n",
       "      <th>sinc5_3</th>\n",
       "      <th>intel5_3</th>\n",
       "      <th>fun5_3</th>\n",
       "      <th>amb5_3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2583</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>372.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6830</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>63.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4840</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>331.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5508</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4828</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>357.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3390</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>214.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4130</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>199.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>290.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5016</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>151.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8149</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>542.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5909 rows √ó 191 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      gender  idg  condtn  wave  round  position  positin1  order  partner  \\\n",
       "id                                                                           \n",
       "2583       0    3       2    14     18         2       2.0     14       12   \n",
       "6830       1   14       1     3     10         2       NaN      8        8   \n",
       "4840       1   14       1    13     10         8       8.0     10       10   \n",
       "5508       1   38       2     9     20        18      13.0      6        7   \n",
       "4828       1   24       2    14     20         6       6.0     20       17   \n",
       "...      ...  ...     ...   ...    ...       ...       ...    ...      ...   \n",
       "3390       0    1       2     9     20         2       2.0     18        1   \n",
       "4130       1   24       2     9     20        19      15.0      5        6   \n",
       "1178       0   13       2    11     21         5       5.0      3       18   \n",
       "5016       1   10       2     7     16         6      14.0      9       10   \n",
       "8149       0    7       2    21     22         7       7.0      2       12   \n",
       "\n",
       "        pid  ...  attr3_3  sinc3_3  intel3_3  fun3_3  amb3_3  attr5_3  \\\n",
       "id           ...                                                        \n",
       "2583  372.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
       "6830   63.0  ...      6.0      8.0       8.0     7.0     8.0      NaN   \n",
       "4840  331.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
       "5508  200.0  ...      8.0      9.0       8.0     8.0     6.0      NaN   \n",
       "4828  357.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
       "...     ...  ...      ...      ...       ...     ...     ...      ...   \n",
       "3390  214.0  ...     12.0     12.0      12.0     9.0    12.0      NaN   \n",
       "4130  199.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
       "1178  290.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
       "5016  151.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
       "8149  542.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
       "\n",
       "      sinc5_3  intel5_3  fun5_3  amb5_3  \n",
       "id                                       \n",
       "2583      NaN       NaN     NaN     NaN  \n",
       "6830      NaN       NaN     NaN     NaN  \n",
       "4840      NaN       NaN     NaN     NaN  \n",
       "5508      NaN       NaN     NaN     NaN  \n",
       "4828      NaN       NaN     NaN     NaN  \n",
       "...       ...       ...     ...     ...  \n",
       "3390      NaN       NaN     NaN     NaN  \n",
       "4130      NaN       NaN     NaN     NaN  \n",
       "1178      NaN       NaN     NaN     NaN  \n",
       "5016      NaN       NaN     NaN     NaN  \n",
       "8149      NaN       NaN     NaN     NaN  \n",
       "\n",
       "[5909 rows x 191 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the training dataset and make the id column in the csv file be the index of the dataframe\n",
    "tr_data = pd.read_csv('train.csv',index_col='id')\n",
    "tr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5909 entries, 2583 to 8149\n",
      "Columns: 191 entries, gender to amb5_3\n",
      "dtypes: float64(173), int64(10), object(8)\n",
      "memory usage: 8.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# view information about the training dataframe\n",
    "tr_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>idg</th>\n",
       "      <th>condtn</th>\n",
       "      <th>wave</th>\n",
       "      <th>round</th>\n",
       "      <th>position</th>\n",
       "      <th>positin1</th>\n",
       "      <th>order</th>\n",
       "      <th>partner</th>\n",
       "      <th>pid</th>\n",
       "      <th>...</th>\n",
       "      <th>attr3_3</th>\n",
       "      <th>sinc3_3</th>\n",
       "      <th>intel3_3</th>\n",
       "      <th>fun3_3</th>\n",
       "      <th>amb3_3</th>\n",
       "      <th>attr5_3</th>\n",
       "      <th>sinc5_3</th>\n",
       "      <th>intel5_3</th>\n",
       "      <th>fun5_3</th>\n",
       "      <th>amb5_3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>52.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6539</th>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>368.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6757</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>212.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2275</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>162.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7982</th>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>407.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7299</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>215.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6691</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>7.0</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>513.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2469 rows √ó 190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      gender  idg  condtn  wave  round  position  positin1  order  partner  \\\n",
       "id                                                                           \n",
       "934        0    5       2     2     16         3       NaN     13       13   \n",
       "6539       0   33       2    14     18         6       6.0      4        8   \n",
       "6757       1    6       2     9     20        10      16.0     15       19   \n",
       "2275       1   26       2     2     19        15       NaN      8       10   \n",
       "1052       0   29       2     7     16         7       7.0     10        5   \n",
       "...      ...  ...     ...   ...    ...       ...       ...    ...      ...   \n",
       "7982       0   23       2    15     19        18      18.0     14       11   \n",
       "7299       0    5       1    13      9         4       4.0      4        8   \n",
       "1818       1   26       2     2     19         3       NaN     15        3   \n",
       "937        0   19       2     9     20        11      11.0      9        2   \n",
       "6691       1   38       2    21     22        22       7.0     16        5   \n",
       "\n",
       "        pid  ...  attr3_3  sinc3_3  intel3_3  fun3_3  amb3_3  attr5_3  \\\n",
       "id           ...                                                        \n",
       "934    52.0  ...      5.0      7.0       8.0     6.0     8.0      NaN   \n",
       "6539  368.0  ...      6.0      8.0       7.0     7.0     8.0      6.0   \n",
       "6757  212.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
       "2275   30.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
       "1052  162.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
       "...     ...  ...      ...      ...       ...     ...     ...      ...   \n",
       "7982  407.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
       "7299  339.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
       "1818   23.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
       "937   215.0  ...      9.0      7.0      12.0    12.0     9.0      NaN   \n",
       "6691  513.0  ...      7.0      9.0       8.0     7.0     8.0      5.0   \n",
       "\n",
       "      sinc5_3  intel5_3  fun5_3  amb5_3  \n",
       "id                                       \n",
       "934       NaN       NaN     NaN     NaN  \n",
       "6539      7.0       6.0     5.0     5.0  \n",
       "6757      NaN       NaN     NaN     NaN  \n",
       "2275      NaN       NaN     NaN     NaN  \n",
       "1052      NaN       NaN     NaN     NaN  \n",
       "...       ...       ...     ...     ...  \n",
       "7982      NaN       NaN     NaN     NaN  \n",
       "7299      NaN       NaN     NaN     NaN  \n",
       "1818      NaN       NaN     NaN     NaN  \n",
       "937       NaN       NaN     NaN     NaN  \n",
       "6691      8.0       8.0     6.0     8.0  \n",
       "\n",
       "[2469 rows x 190 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the test dataset and make the id column in the csv file be the index of the dataframe\n",
    "ts_data = pd.read_csv('test.csv',index_col='id')\n",
    "ts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape (5909, 190) (5909,)\n"
     ]
    }
   ],
   "source": [
    "# split the training data to labels and features to be used in the supervised learning :)\n",
    "# get the match column as it is our label column\n",
    "y = tr_data['match']\n",
    "# drop the match column and take the rest of the dataframe to be our features\n",
    "X = tr_data.drop('match', axis=1) \n",
    "# print the shapes of both labels and features\n",
    "print('original shape', X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>idg</th>\n",
       "      <th>condtn</th>\n",
       "      <th>wave</th>\n",
       "      <th>round</th>\n",
       "      <th>position</th>\n",
       "      <th>positin1</th>\n",
       "      <th>order</th>\n",
       "      <th>partner</th>\n",
       "      <th>pid</th>\n",
       "      <th>...</th>\n",
       "      <th>attr3_3</th>\n",
       "      <th>sinc3_3</th>\n",
       "      <th>intel3_3</th>\n",
       "      <th>fun3_3</th>\n",
       "      <th>amb3_3</th>\n",
       "      <th>attr5_3</th>\n",
       "      <th>sinc5_3</th>\n",
       "      <th>intel5_3</th>\n",
       "      <th>fun5_3</th>\n",
       "      <th>amb5_3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2583</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>372.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6830</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>63.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4840</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>331.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5508</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4828</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>357.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      gender  idg  condtn  wave  round  position  positin1  order  partner  \\\n",
       "id                                                                           \n",
       "2583       0    3       2    14     18         2       2.0     14       12   \n",
       "6830       1   14       1     3     10         2       NaN      8        8   \n",
       "4840       1   14       1    13     10         8       8.0     10       10   \n",
       "5508       1   38       2     9     20        18      13.0      6        7   \n",
       "4828       1   24       2    14     20         6       6.0     20       17   \n",
       "\n",
       "        pid  ...  attr3_3  sinc3_3  intel3_3  fun3_3  amb3_3  attr5_3  \\\n",
       "id           ...                                                        \n",
       "2583  372.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
       "6830   63.0  ...      6.0      8.0       8.0     7.0     8.0      NaN   \n",
       "4840  331.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
       "5508  200.0  ...      8.0      9.0       8.0     8.0     6.0      NaN   \n",
       "4828  357.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
       "\n",
       "      sinc5_3  intel5_3  fun5_3  amb5_3  \n",
       "id                                       \n",
       "2583      NaN       NaN     NaN     NaN  \n",
       "6830      NaN       NaN     NaN     NaN  \n",
       "4840      NaN       NaN     NaN     NaN  \n",
       "5508      NaN       NaN     NaN     NaN  \n",
       "4828      NaN       NaN     NaN     NaN  \n",
       "\n",
       "[5 rows x 190 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make concatinating for the two dataframes so I can make the data preparation steps once on both of them then after finishing\n",
    "# the data preparation I split them again, make the concatinate by keeping the id in the same order so we can split again \n",
    "# both datasets according to the id\n",
    "frames = [X, ts_data]\n",
    "result = pd.concat(frames)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8378 entries, 2583 to 6691\n",
      "Columns: 190 entries, gender to amb5_3\n",
      "dtypes: float64(173), int64(9), object(8)\n",
      "memory usage: 12.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# view information about both training and test features after concataining them\n",
    "result.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation üî≠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make function to calculate the percentage and number of the nan values in each column in the dataframe\n",
    "def missing_v(df):\n",
    "    missing = pd.DataFrame(df.isnull().sum()/len(df))*100\n",
    "    missing.columns = ['missing_values(%)']\n",
    "    missing['missing_values(numbers)'] = pd.DataFrame(df.isnull().sum())\n",
    "    return missing.sort_values(by = 'missing_values(%)', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_values(%)</th>\n",
       "      <th>missing_values(numbers)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>num_in_3</th>\n",
       "      <td>92.026737</td>\n",
       "      <td>7710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numdat_3</th>\n",
       "      <td>82.143710</td>\n",
       "      <td>6882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expnum</th>\n",
       "      <td>78.515159</td>\n",
       "      <td>6578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sinc7_2</th>\n",
       "      <td>76.665075</td>\n",
       "      <td>6423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amb7_2</th>\n",
       "      <td>76.665075</td>\n",
       "      <td>6423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idg</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>order</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partner</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>samerace</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          missing_values(%)  missing_values(numbers)\n",
       "num_in_3          92.026737                     7710\n",
       "numdat_3          82.143710                     6882\n",
       "expnum            78.515159                     6578\n",
       "sinc7_2           76.665075                     6423\n",
       "amb7_2            76.665075                     6423\n",
       "...                     ...                      ...\n",
       "idg                0.000000                        0\n",
       "order              0.000000                        0\n",
       "partner            0.000000                        0\n",
       "samerace           0.000000                        0\n",
       "gender             0.000000                        0\n",
       "\n",
       "[190 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the percentage and number of the nan values in the features dataframe \n",
    "result_mis = missing_v(result)\n",
    "result_mis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['num_in_3',\n",
       " 'numdat_3',\n",
       " 'expnum',\n",
       " 'sinc7_2',\n",
       " 'amb7_2',\n",
       " 'shar7_2',\n",
       " 'intel7_2',\n",
       " 'attr7_2',\n",
       " 'fun7_2',\n",
       " 'attr5_3',\n",
       " 'shar7_3',\n",
       " 'fun5_3',\n",
       " 'intel5_3',\n",
       " 'sinc5_3',\n",
       " 'attr7_3',\n",
       " 'sinc7_3',\n",
       " 'intel7_3',\n",
       " 'fun7_3',\n",
       " 'amb7_3',\n",
       " 'amb5_3',\n",
       " 'shar2_3']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loop on the all columns in the dataframe that contains the percentage values of nan values and fill a list with columns' names\n",
    "# that contain more than 70%  missing values\n",
    "empty_col = []\n",
    "for i in range (len(result_mis)):\n",
    "    if (result_mis['missing_values(%)'][i]) > 70:\n",
    "#         print(tr_mis['missing_values(%)'][i])\n",
    "        empty_col.append(result_mis['missing_values(%)'].index[i])\n",
    "empty_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field</th>\n",
       "      <th>undergra</th>\n",
       "      <th>mn_sat</th>\n",
       "      <th>tuition</th>\n",
       "      <th>from</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>income</th>\n",
       "      <th>career</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2583</th>\n",
       "      <td>Ed.D. in higher education policy at TC</td>\n",
       "      <td>University of Michigan-Ann Arbor</td>\n",
       "      <td>1,290.00</td>\n",
       "      <td>21,645.00</td>\n",
       "      <td>Palo Alto, CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>University President</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6830</th>\n",
       "      <td>Engineering</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>2,021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Engineer or iBanker or consultant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4840</th>\n",
       "      <td>Urban Planning</td>\n",
       "      <td>Rizvi College of Architecture, Bombay University</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bombay, India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Real Estate Consulting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5508</th>\n",
       "      <td>International Affairs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>10,471</td>\n",
       "      <td>45,300.00</td>\n",
       "      <td>public service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4828</th>\n",
       "      <td>Business</td>\n",
       "      <td>Harvard College</td>\n",
       "      <td>1,400.00</td>\n",
       "      <td>26,019.00</td>\n",
       "      <td>Midwest USA</td>\n",
       "      <td>66,208</td>\n",
       "      <td>46,138.00</td>\n",
       "      <td>undecided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7982</th>\n",
       "      <td>Neuroscience and Education</td>\n",
       "      <td>Columbia</td>\n",
       "      <td>1,430.00</td>\n",
       "      <td>26,908.00</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Academic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7299</th>\n",
       "      <td>School Psychology</td>\n",
       "      <td>Bucknell University</td>\n",
       "      <td>1,290.00</td>\n",
       "      <td>25,335.00</td>\n",
       "      <td>Erie, PA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>school psychologist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>Law</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>11,204</td>\n",
       "      <td>26,482.00</td>\n",
       "      <td>Intellectual Property Attorney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>Mathematics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vestal</td>\n",
       "      <td>13,850</td>\n",
       "      <td>42,640.00</td>\n",
       "      <td>college professor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6691</th>\n",
       "      <td>General management/finance</td>\n",
       "      <td>LUISS, Rome</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Italy</td>\n",
       "      <td>136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General management/consulting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8378 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       field  \\\n",
       "id                                             \n",
       "2583  Ed.D. in higher education policy at TC   \n",
       "6830                             Engineering   \n",
       "4840                          Urban Planning   \n",
       "5508                   International Affairs   \n",
       "4828                                Business   \n",
       "...                                      ...   \n",
       "7982              Neuroscience and Education   \n",
       "7299                       School Psychology   \n",
       "1818                                     Law   \n",
       "937                              Mathematics   \n",
       "6691              General management/finance   \n",
       "\n",
       "                                              undergra    mn_sat    tuition  \\\n",
       "id                                                                            \n",
       "2583                  University of Michigan-Ann Arbor  1,290.00  21,645.00   \n",
       "6830                                               NaN       NaN        NaN   \n",
       "4840  Rizvi College of Architecture, Bombay University       NaN        NaN   \n",
       "5508                                               NaN       NaN        NaN   \n",
       "4828                                   Harvard College  1,400.00  26,019.00   \n",
       "...                                                ...       ...        ...   \n",
       "7982                                          Columbia  1,430.00  26,908.00   \n",
       "7299                               Bucknell University  1,290.00  25,335.00   \n",
       "1818                                               NaN       NaN        NaN   \n",
       "937                                                NaN       NaN        NaN   \n",
       "6691                                       LUISS, Rome       NaN        NaN   \n",
       "\n",
       "                from zipcode     income                             career  \n",
       "id                                                                          \n",
       "2583   Palo Alto, CA     NaN        NaN               University President  \n",
       "6830      Boston, MA   2,021        NaN  Engineer or iBanker or consultant  \n",
       "4840   Bombay, India     NaN        NaN             Real Estate Consulting  \n",
       "5508  Washington, DC  10,471  45,300.00                     public service  \n",
       "4828     Midwest USA  66,208  46,138.00                          undecided  \n",
       "...              ...     ...        ...                                ...  \n",
       "7982       Hong Kong       0        NaN                           Academic  \n",
       "7299        Erie, PA     NaN        NaN                school psychologist  \n",
       "1818        Brooklyn  11,204  26,482.00     Intellectual Property Attorney  \n",
       "937           Vestal  13,850  42,640.00                  college professor  \n",
       "6691           Italy     136        NaN      General management/consulting  \n",
       "\n",
       "[8378 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all columns that have tpye: object as we need to convert the object type to categorical data type so we can deal with it\n",
    "result.select_dtypes(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make copy from the dataframe so we can return to the original data frame if we made a mistake so we don't read the file again\n",
    "df2 = result.copy()\n",
    "# convert the object data type to categorical data type\n",
    "# I won't make convert for the field, and career columns as they are redundant and I will drop them \n",
    "# categorical encoding of undergra\n",
    "df2['undergra'] = result['undergra'].astype(\"category\")\n",
    "\n",
    "# categorical encoding of mn_sat location\n",
    "df2['mn_sat'] = result['mn_sat'].astype(\"category\")\n",
    "\n",
    "# use the first letter in tuition\n",
    "df2['tuition'] = result['tuition'].astype(\"category\")\n",
    "\n",
    "# use the first letter in from\n",
    "df2['from'] = result['from'].astype(\"category\")\n",
    "\n",
    "# use the first letter in from\n",
    "df2['zipcode'] = result['zipcode'].astype(\"category\")\n",
    "\n",
    "# use the first letter in income\n",
    "df2['income'] = result['income'].astype(\"category\")\n",
    "\n",
    "# drop field, career columns (As they are redundant columns and there are two columns describe the content in numerical way)\n",
    "# axis=1 indicates that we drop vertically\n",
    "# drop the columns that contain missing values more than 70% \n",
    "df2 = df2.drop(['field','career', 'num_in_3', 'numdat_3', 'expnum', 'sinc7_2', 'amb7_2', 'shar7_2', 'intel7_2', 'attr7_2',\n",
    "                 'fun7_2', 'attr5_3', 'shar7_3', 'fun5_3', 'intel5_3', 'sinc5_3', 'attr7_3', 'sinc7_3', 'intel7_3', 'fun7_3',\n",
    "                 'amb7_3', 'amb5_3', 'shar2_3'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8378 entries, 2583 to 6691\n",
      "Columns: 167 entries, gender to amb3_3\n",
      "dtypes: category(6), float64(152), int64(9)\n",
      "memory usage: 10.5 MB\n"
     ]
    }
   ],
   "source": [
    "# view information about the dataframe after filling the nan values, drop some columns, convert object columns to categorical\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of new dataframes - (5909, 167) , (2469, 167)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>idg</th>\n",
       "      <th>condtn</th>\n",
       "      <th>wave</th>\n",
       "      <th>round</th>\n",
       "      <th>position</th>\n",
       "      <th>positin1</th>\n",
       "      <th>order</th>\n",
       "      <th>partner</th>\n",
       "      <th>pid</th>\n",
       "      <th>...</th>\n",
       "      <th>attr2_3</th>\n",
       "      <th>sinc2_3</th>\n",
       "      <th>intel2_3</th>\n",
       "      <th>fun2_3</th>\n",
       "      <th>amb2_3</th>\n",
       "      <th>attr3_3</th>\n",
       "      <th>sinc3_3</th>\n",
       "      <th>intel3_3</th>\n",
       "      <th>fun3_3</th>\n",
       "      <th>amb3_3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>52.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6539</th>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>368.0</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6757</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>212.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2275</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>162.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      gender  idg  condtn  wave  round  position  positin1  order  partner  \\\n",
       "id                                                                           \n",
       "934        0    5       2     2     16         3       NaN     13       13   \n",
       "6539       0   33       2    14     18         6       6.0      4        8   \n",
       "6757       1    6       2     9     20        10      16.0     15       19   \n",
       "2275       1   26       2     2     19        15       NaN      8       10   \n",
       "1052       0   29       2     7     16         7       7.0     10        5   \n",
       "\n",
       "        pid  ...  attr2_3  sinc2_3  intel2_3  fun2_3  amb2_3  attr3_3  \\\n",
       "id           ...                                                        \n",
       "934    52.0  ...      NaN      NaN       NaN     NaN     NaN      5.0   \n",
       "6539  368.0  ...     30.0     10.0       0.0    30.0     0.0      6.0   \n",
       "6757  212.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
       "2275   30.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
       "1052  162.0  ...      NaN      NaN       NaN     NaN     NaN      NaN   \n",
       "\n",
       "      sinc3_3  intel3_3  fun3_3  amb3_3  \n",
       "id                                       \n",
       "934       7.0       8.0     6.0     8.0  \n",
       "6539      8.0       7.0     7.0     8.0  \n",
       "6757      NaN       NaN     NaN     NaN  \n",
       "2275      NaN       NaN     NaN     NaN  \n",
       "1052      NaN       NaN     NaN     NaN  \n",
       "\n",
       "[5 rows x 167 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting dataframe by row index after finishing data preparation to return again to training features and test features\n",
    "X = df2.iloc[:5909 ,:]\n",
    "df2_ts = df2.iloc[5909:,:]\n",
    "# print the shape of each part to make sure that the shapes are right\n",
    "print(\"Shape of new dataframes - {} , {}\".format(X.shape, df2_ts.shape))\n",
    "# print the top 5 rows in the test dataframe to make sure that it starts with the right ID after splitting the result dataframe\n",
    "df2_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numeric features: ['gender', 'idg', 'condtn', 'wave', 'round', 'position', 'positin1', 'order', 'partner', 'pid', 'int_corr', 'samerace', 'age_o', 'race_o', 'pf_o_att', 'pf_o_sin', 'pf_o_int', 'pf_o_fun', 'pf_o_amb', 'pf_o_sha', 'attr_o', 'sinc_o', 'intel_o', 'fun_o', 'amb_o', 'shar_o', 'like_o', 'prob_o', 'met_o', 'age', 'field_cd', 'race', 'imprace', 'imprelig', 'goal', 'date', 'go_out', 'career_c', 'sports', 'tvsports', 'exercise', 'dining', 'museums', 'art', 'hiking', 'gaming', 'clubbing', 'reading', 'tv', 'theater', 'movies', 'concerts', 'music', 'shopping', 'yoga', 'exphappy', 'attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1', 'attr4_1', 'sinc4_1', 'intel4_1', 'fun4_1', 'amb4_1', 'shar4_1', 'attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1', 'attr3_1', 'sinc3_1', 'fun3_1', 'intel3_1', 'amb3_1', 'attr5_1', 'sinc5_1', 'intel5_1', 'fun5_1', 'amb5_1', 'attr', 'sinc', 'intel', 'fun', 'amb', 'shar', 'like', 'prob', 'met', 'match_es', 'attr1_s', 'sinc1_s', 'intel1_s', 'fun1_s', 'amb1_s', 'shar1_s', 'attr3_s', 'sinc3_s', 'intel3_s', 'fun3_s', 'amb3_s', 'satis_2', 'length', 'numdat_2', 'attr1_2', 'sinc1_2', 'intel1_2', 'fun1_2', 'amb1_2', 'shar1_2', 'attr4_2', 'sinc4_2', 'intel4_2', 'fun4_2', 'amb4_2', 'shar4_2', 'attr2_2', 'sinc2_2', 'intel2_2', 'fun2_2', 'amb2_2', 'shar2_2', 'attr3_2', 'sinc3_2', 'intel3_2', 'fun3_2', 'amb3_2', 'attr5_2', 'sinc5_2', 'intel5_2', 'fun5_2', 'amb5_2', 'you_call', 'them_cal', 'date_3', 'attr1_3', 'sinc1_3', 'intel1_3', 'fun1_3', 'amb1_3', 'shar1_3', 'attr4_3', 'sinc4_3', 'intel4_3', 'fun4_3', 'amb4_3', 'shar4_3', 'attr2_3', 'sinc2_3', 'intel2_3', 'fun2_3', 'amb2_3', 'attr3_3', 'sinc3_3', 'intel3_3', 'fun3_3', 'amb3_3']\n",
      "categorical features: ['undergra', 'mn_sat', 'tuition', 'from', 'zipcode', 'income']\n"
     ]
    }
   ],
   "source": [
    "# we extract numeric features and categorical features names\n",
    "# numeric features can be selected by get the column that have data type float and int from the training data frame\n",
    "features_numeric = list(X.select_dtypes(include=['float64', 'int64']))\n",
    "\n",
    "# categorical features can be selected by get the column that have data type categorical from the training data frame \n",
    "features_categorical = list(X.select_dtypes(include=['category']))\n",
    "\n",
    "# print names of the features numeric and categorical\n",
    "print('numeric features:', features_numeric)\n",
    "print('categorical features:', features_categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start in creating the pipeline üë∑\n",
    "\n",
    "\n",
    "**First I will make the fix the random seed by 0 so, it will generate the same random numbers each time I run the code.**\n",
    "\n",
    "**Then the pipeline needs a preprocessor, and a classifier, so I will talk about the preprocessor first. In the preprocessor, I need to deal with two types in the datasets \"numerical data types, and categorical data types\". So, I will create two pipelines, each one to deal with one of the data types.**\n",
    "\n",
    "**First the numerical pipeline üî¢, I use the imputer as 'SimpleImputer' to deal with the missing values as it makes a transform to complete the missing values, and its parameter will be passed from each callback for the pipeline in each trial. I use the scaler as 'StandardScaler' so it will make normalization for the numerical features by removing the mean and scaling to unit variance for each numerical feature.**\n",
    "\n",
    "\n",
    "**Second the categorical pipeline üî†, I use the imputer as 'SimpleImputer' to deal with the missing values as it makes a transform to complete the missing values, and its parameter is 'constant' so it will fill the missing value by ‚Äúmissing_value‚Äù as here it fills categorical data not numerical, if numerical it will fill them by '0'. Then I use OneHotEncoder method to convert the categorical columns into numerical, and ignore it there any unknown features in the train dataset or test dataset. As there may be unknown features in one of the two datasets contains more categorical values than the other; so after converting them may one contains more numerical features than the other, and this causes errors, so I will ignore this.**\n",
    "\n",
    "**So, the processor contains both pipelines one for the numerical features and one for the categorical features so it will transform the columns according to the pipeline which the column related to.**\n",
    "\n",
    "**Then, as the full pipeline contains a preprocessor and classifier, so the preprocessor I talked about and first I will use the 'XGBClassifier'. As the XGBClassifier is efficient, flexible, and portable. It provides parallel tree boosting (also known as GBDT, GBM) that solves many data science problems in a fast and accurate way.**\n",
    "\n",
    "\n",
    "**In the other pipeline, I use Logistic regression classifier as it is one of the best classifiers to be used in binary classification. I will make it handle the unbalance between the weights of each class.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the preprocessor which will be used in the pipeline\n",
    "# make any random generations be the same by each run for the code\n",
    "np.random.seed(0)\n",
    "\n",
    "# define a pipe line for numeric feature preprocessing\n",
    "# we gave them a name so we can set their hyperparameters\n",
    "transformer_numeric = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer()),\n",
    "        ('scaler', StandardScaler())]\n",
    ")\n",
    "\n",
    "# define a pipe line for categorical feature preprocessing\n",
    "# we gave them a name so we can set their hyperparameters\n",
    "transformer_categorical = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# define the preprocessor \n",
    "# we gave them a name so we can set their hyperparameters\n",
    "# we also specify what are the categorical \n",
    "# pass the numerical features and categorical features for each transformer 'pipeline'\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', transformer_numeric, features_numeric),\n",
    "        ('cat', transformer_categorical, features_categorical)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['gender', 'idg', 'condtn',\n",
       "                                                   'wave', 'round', 'position',\n",
       "                                                   'positin1', 'order',\n",
       "                                                   'partner', 'pid', 'int_corr',\n",
       "                                                   'samerace', 'age_o',\n",
       "                                                   'race_o', 'pf_o_att',\n",
       "                                                   'pf_o_sin', 'pf_o_int',\n",
       "                                                   'pf_o_fun', 'pf_o_amb',\n",
       "                                                   'pf_o_sha', 'attr_o',\n",
       "                                                   'sinc...\n",
       "                               interaction_constraints=None, learning_rate=None,\n",
       "                               max_delta_step=None, max_depth=None,\n",
       "                               min_child_weight=None, missing=nan,\n",
       "                               monotone_constraints=None, n_estimators=100,\n",
       "                               n_jobs=None, num_parallel_tree=None,\n",
       "                               predictor=None, random_state=None,\n",
       "                               reg_alpha=None, reg_lambda=None,\n",
       "                               scale_pos_weight=None, subsample=None,\n",
       "                               tree_method=None, validate_parameters=None,\n",
       "                               verbosity=None))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create pipeline for XGBClassifier\n",
    "# combine the preprocessor with the model as a full tunable pipeline\n",
    "# we gave them a name so we can set their hyperparameters\n",
    "full_pipline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('my_classifier', \n",
    "           XGBClassifier(),\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "full_pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['gender', 'idg', 'condtn',\n",
       "                                                   'wave', 'round', 'position',\n",
       "                                                   'positin1', 'order',\n",
       "                                                   'partner', 'pid', 'int_corr',\n",
       "                                                   'samerace', 'age_o',\n",
       "                                                   'race_o', 'pf_o_att',\n",
       "                                                   'pf_o_sin', 'pf_o_int',\n",
       "                                                   'pf_o_fun', 'pf_o_amb',\n",
       "                                                   'pf_o_sha', 'attr_o',\n",
       "                                                   'sinc_o', 'intel_o', 'fun_o',\n",
       "                                                   'amb_o', 'shar_o', 'like_o',\n",
       "                                                   'prob_o', 'met_o', 'age', ...]),\n",
       "                                                 ('cat',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(strategy='constant')),\n",
       "                                                                  ('onehot',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  ['undergra', 'mn_sat',\n",
       "                                                   'tuition', 'from', 'zipcode',\n",
       "                                                   'income'])])),\n",
       "                ('my_classifier', LogisticRegression(class_weight='balanced'))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create another pipeline with change in the classifier only and use the same preprocessor\n",
    "# In the LogisticRegression I will make it makes balance between the weights of each class\n",
    "# combine the preprocessor with the model as a full tunable pipeline\n",
    "# we gave them a name so we can set their hyperparameters\n",
    "full_pipline2 = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('my_classifier', \n",
    "           LogisticRegression(class_weight=\"balanced\"),\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "full_pipline2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trials üèÉ‚Äç‚ôÄÔ∏è ü§ó\n",
    "* I use 6 trials as the following:\n",
    "    1. Grid Search with Cross-validation with xgboost classifier\n",
    "    2. Grid Search with Cross-validation with LogisticRegression classifier\n",
    "    3. Grid Search with validation set with xgboost classifier\n",
    "    4. Grid Search with validation set with LogisticRegression classifier\n",
    "    5. Random Search with xgboost classifier\n",
    "    6. Bayesian Search with xgboost classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Grid Search with Cross-validation with xgboost classifier\n",
    "* Here I will use cross-validation as the experimental protocol, so in this method the training set is divided into training and test datasets and in each iteration the test dataset changes. So, here I will use cross-validation equal 5, so the training dataset will splitted into 5 parts, and in each iteration the model will use 4 from them as training dataset and 1 for the test the model and get the score according to the metric I will use, which is = roc_auc.\n",
    "* I use a grid to put the ranges of the hyperparameters for each model. So, in the imputer for the numerical pipeline I will pass 'mean' and 'most_frequent'.So, it will fill the missing values in the columns of the training and test whether by the mean of the numbers in each column or the most repeated number in each column.\n",
    "* In the number of the estimators, which refers to the number of gradient boosted trees that will be used in the model, I will pass different numbers for it from small number to big number to see what the best number of estimators is.\n",
    "* In the max depth, which refers to the maximum tree depth for the base learners, I will use small different values, so by using small numbers, this won't make overfitting in the training phase.\n",
    "\n",
    "\n",
    "* Here I use grid search so the grid search makes combinations of all hyperparameters given and try all of them in the fitting of the training dataset with their labels; then it gives the best combination of the hyperparameters that will separate the two classes in the best way.\n",
    "\n",
    "\n",
    "* Then in the test phase I use the combination that gave the best result in the training phase to predict the probability of each class and save the probability of the value '1' that means it will match to test the results on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 56 candidates, totalling 280 fits\n",
      "[00:21:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "best score 0.8810673915129037\n",
      "best score {'my_classifier__max_depth': 5, 'my_classifier__n_estimators': 80, 'preprocessor__num__imputer__strategy': 'mean'}\n"
     ]
    }
   ],
   "source": [
    "# Grid Search with Cross-validation with XGBoost classifier\n",
    "\n",
    "# here we specify the search space for this classifier and type for the search for the hyperparameters\n",
    "# `__` denotes an attribute of the preceeding name\n",
    "# so according to this, my_classifier__max_depth means the attribute max_depth for the classifier in the pipeline used and so on\n",
    "param_grid = {\n",
    "    # preprocessor__num__imputer__strategy points to preprocessor->num (a Pipeline)-> imputer -> strategy\n",
    "    'preprocessor__num__imputer__strategy': ['mean', 'most_frequent'],\n",
    "    # my_classifier__n_estimators points to my_classifier->n_estimators \n",
    "    'my_classifier__n_estimators': [80, 100, 200, 300, 400, 500, 600],  \n",
    "     # my_classifier__max_depth points to my_classifier->max_depth\n",
    "    'my_classifier__max_depth':[5, 7, 10, 15]       \n",
    "}\n",
    "\n",
    "# cv=5 means five-folds cross-validation\n",
    "# n_jobs means the cucurrent number of jobs\n",
    "# (on colab since we only have two cpu cores, we set it to 2)\n",
    "grid_search = GridSearchCV(\n",
    "    full_pipline, param_grid, cv=5, verbose=1, n_jobs=2, \n",
    "    scoring='roc_auc')\n",
    "\n",
    "# make fitting on the training dataset and their labels to get the best combination of the hyperparameters\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# print the best score and the combination of the hyperparameters that got that score\n",
    "print('best score {}'.format(grid_search.best_score_))\n",
    "print('best score {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the results and saving in csv file \n",
    "submission = pd.DataFrame()\n",
    "# get the id from the test dataframe\n",
    "submission['id'] = ts_data.index\n",
    "\n",
    "# get the predict probability of the match value '1' by the combination of the hyperparameters that fitted the model good in the\n",
    "# training phase\n",
    "submission['match'] = grid_search.predict_proba(df2_ts)[:,1]\n",
    "\n",
    "submission.to_csv('GS_CV.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts and observations for trial 1: ü§ì\n",
    "This gave score on Kaggle: 0.87355\n",
    "\n",
    "This model made 280 iterations in the fitting as there are 56 different combinations from the hyperparameters and the cross-validation = 5 so the k-folds in the training and test phases will change 5 times and tried on the 56 combinations, so I got 280 fitting iterations.\n",
    "\n",
    "This model used hyperparameters as following:'max_depth': 5, 'n_estimators': 80, 'imputer__strategy': 'mean', this means that the imputer in the numerical pipeline will fill the missing values in the numerical features (columns) with the mean of each feature (column) in the training dataframe and test dataframe. Besides, the classifier will use number of estimators = 80, which means that the model is compiled with 80 gradient boosted trees and max depth for each tree = 5, which is a small number, so I thought this won't make overfitting on the training dataset. The grid search on the cross-validation phase on the training dataset gave AUC score = 0.88106, and on Kaggle gave 0.87355; so this means that this combination from the hyperparameters didn't make overfitting on the training dataset as on the unseen dataset (test dataset) this model gave a very good score.\n",
    "\n",
    "\n",
    "### Plan for trial 2: ü§î\n",
    "I will change the classifier in the next trial and I will use Logistic Regression classifier with continue using the cross-validation way but this time I will use 2 cross validations so the whole training dataset will divide into two parts (training and test) and this will change in the 2 iterations. Adjusting the  hyperparameters: solver, penalty, and C value using the grid search method, which I talked about before. As the solver means the algorithm that will be uses in the optimization (if the dataset is small ‚Äòliblinear‚Äô is a good choice, for multiclass problems ‚Äònewton-cg‚Äô, ‚Äòlbfgs‚Äô handle multinomial loss, and the default for the model is ‚Äòlbfgs‚Äô), the regularization means the regularization method that will be used on the weights and the parameters of the model that is used to prevent the overfitting on the training dataset, and C to control with the penality strength of the regularization as smaller values specify stronger regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Grid Search with Cross-validation with LogisticRegression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n",
      "best score 0.8579257126421844\n",
      "best score {'my_classifier__C': 0.01, 'my_classifier__penalty': 'l2', 'my_classifier__solver': 'lbfgs', 'preprocessor__num__imputer__strategy': 'mean'}\n"
     ]
    }
   ],
   "source": [
    "# Grid Search with Cross-validation with LogisticRegression classifier\n",
    "\n",
    "# here we specify the search space\n",
    "# `__` denotes an attribute of the preceeding name\n",
    "# so according to this, my_classifier__max_depth means the attribute max_depth for the classifier in the pipeline used and so on\n",
    "param_grid2 = {\n",
    "    # preprocessor__num__imputer__strategy points to preprocessor->num (a Pipeline)-> imputer -> strategy\n",
    "    'preprocessor__num__imputer__strategy': ['mean', 'most_frequent'],\n",
    "    # my_classifier__solver points to my_classifier->solver \n",
    "    'my_classifier__solver' : ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "     # my_classifier__penalty points to my_classifier->penalty of the regularization to prevent the overfitting in the model  \n",
    "    'my_classifier__penalty': ['l2'],\n",
    "      # my_classifier__C points to my_classifier->C values \n",
    "    'my_classifier__C' : [1000, 100, 10, 1.0, 0.1, 0.01]\n",
    "}\n",
    "\n",
    "# cv=2 means two-fold cross-validation\n",
    "# n_jobs means the cucurrent number of jobs\n",
    "# (on colab since we only have two cpu cores, we set it to 2)\n",
    "grid_search_LR = GridSearchCV(\n",
    "    full_pipline2, param_grid2, cv=2, verbose=1, n_jobs=2, \n",
    "    scoring='roc_auc')\n",
    "\n",
    "# make fitting on the training dataset and their labels to get the best combination of the hyperparameters\n",
    "grid_search_LR.fit(X, y)\n",
    "\n",
    "# print the best score and the combination of the hyperparameters that got that score\n",
    "print('best score {}'.format(grid_search_LR.best_score_))\n",
    "print('best score {}'.format(grid_search_LR.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the results and saving in csv file \n",
    "submission = pd.DataFrame()\n",
    "# get the id from the test dataframe\n",
    "submission['id'] = ts_data.index\n",
    "\n",
    "# get the predict probability of the match value '1' by the combination of the hyperparameters that fitted the model good in the\n",
    "# training phase\n",
    "submission['match'] = grid_search_LR.predict_proba(df2_ts)[:,1]\n",
    "\n",
    "submission.to_csv('GS_CV_LR.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts and observations  for trial 2: üßê\n",
    "This gave score on Kaggle: 0.86746\n",
    "\n",
    "This model made 72 iterations in the fitting as there are 36 different combinations from the hyperparameters and the cross-validation = 2, so the k-folds in the training, and test phases will change 2 times and tried on the 36 combinations, so I got 72 fitting iterations.\n",
    "\n",
    "This model used hyperparameters as following: 'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs', 'imputer__strategy': 'mean', this means that the imputer in the numerical pipeline will fill the missing values in the numerical features (columns) with the mean of each feature (column) in the training dataframe and test dataframe. Besides, the classifier will use solver = 'lbfgs', which means that the model is using the default solver as the algorithm to solve the optimization and penalty for the weights and the parameters = L2, which means the weights and the parameters of the model will be smaller. The C value is 0.01, which makes the regularization stronger. The grid search on the cross-validation phase on the training dataset gave AUC score = 0.85792, and on Kaggle gave 0.86746; so this means that this combination from the hyperparameters with logistic regression model didn't make overfitting on the training dataset as on the unseen dataset (test dataset) this model gave a very good score, but less than the XGBoost classifier, as we mentioned before, the XGBoost is better classifier than any classifier else.\n",
    "\n",
    "\n",
    "### Plan for trial 3: ü§®\n",
    "I will return to using the XGBoost classifier in the next trial. Using the grid parameters I used for the first time with the XGBoost classifier. But this time I will use the holdout method by splitting the training dataset into training and validation datasets. Besides, I keep using the grid search method to get the best combination of the hyperparameters that will give best AUC score when testing on the validation dataset. I won't change anything in the hyperparameters so I can see how cross-validation and holdout methods make a difference from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Grid Search with validation set with xgboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training features and labels into training and validation dataset with stratify the label so each part will contain\n",
    "# suitable number of each label \n",
    "X_train2, X_val, y_train2, y_val = train_test_split(\n",
    "    X, y, train_size = 0.9, stratify = y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 56 candidates, totalling 56 fits\n",
      "[00:27:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "best score 0.911102898907777\n",
      "best score {'my_classifier__max_depth': 5, 'my_classifier__n_estimators': 100, 'preprocessor__num__imputer__strategy': 'most_frequent'}\n"
     ]
    }
   ],
   "source": [
    "# Create a list where train data indices are -1 and validation data indices are 0\n",
    "# X_train2 (new training set), X_train\n",
    "split_index = [-1 if x in X_train2.index else 0 for x in X.index]\n",
    "\n",
    "# Use the list to create PredefinedSplit\n",
    "pds = PredefinedSplit(test_fold = split_index)\n",
    "\n",
    "grid_search_val = GridSearchCV(\n",
    "    full_pipline, param_grid, cv=pds, verbose=1, n_jobs=2, \n",
    "    scoring='roc_auc')\n",
    "\n",
    "# here we still use X_train; but the grid search model\n",
    "# will use our predefined split internally to determine \n",
    "# which sample belongs to the validation set\n",
    "# make fitting on the training dataset and their labels to get the best combination of the hyperparameters\n",
    "grid_search_val.fit(X, y)\n",
    "\n",
    "# print the best score and the combination of the hyperparameters that got that score\n",
    "print('best score {}'.format(grid_search_val.best_score_))\n",
    "print('best score {}'.format(grid_search_val.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the results and saving in csv file \n",
    "submission = pd.DataFrame()\n",
    "# get the id from the test dataframe\n",
    "submission['id'] = ts_data.index\n",
    "\n",
    "# get the predict probability of the match value '1' by the combination of the hyperparameters that fitted the model good in the\n",
    "# training phase\n",
    "submission['match'] = grid_search_val.predict_proba(df2_ts)[:,1]\n",
    "\n",
    "submission.to_csv('GS_Val.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts and observations for trial 3: ‚úå\n",
    "This gave score on Kaggle: 0.87976\n",
    "\n",
    "This model made 56 iterations in the fitting as there are 56 different combinations from the hyperparameters and all of them will be tried while fitting on the part of the training dataset and tested on the validation dataset, so this will give 56 total iterations.\n",
    "\n",
    "This model used hyperparameters as following:'max_depth': 5, 'n_estimators': 100, 'imputer__strategy': 'most_frequent', this means that the imputer in the numerical pipeline will fill the missing values in the numerical features (columns) with the most value happen in each feature (column) in the training dataframe and test dataframe. Besides, the classifier will use number of estimators = 100, which means that the model is compiled with 100 gradient boosted trees and max depth for each tree = 5, which is a small number, so I thought this won't make overfitting on the training dataset. The grid search on the holdout phase on the training dataset gave AUC score = 0.91110 and on Kaggle gave 0.87976, so I think there is small overfitting on the training dataset as on the unseen dataset (test dataset) this model gave a very good score, but less than what I expected after I got 0.91110 score from the holdout method. But this is better from the first trial using the same classifier, so this means changing the way in the experimental protocol that made changes in the imputer, and number of estimators, all of this made the model got better predictions on the test dataset.\n",
    "\n",
    "### Plan for trial 4: ü§ï\n",
    "I will change the classifier in the next trial and I will return to using Logistic Regression classifier, but this time using the hold out method not the cross-validation. Using the same grid for the hyperparameters: solver, penalty, and C value using the grid search method which I talked about before to get the best combination using the holdout method. I mentioned the rule of each hyperparameter before when I used this classifier with the cross-validation way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Grid Search with validation set with LogisticRegression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 36 candidates, totalling 36 fits\n",
      "best score 0.9012687854151268\n",
      "best score {'my_classifier__C': 0.01, 'my_classifier__penalty': 'l2', 'my_classifier__solver': 'liblinear', 'preprocessor__num__imputer__strategy': 'most_frequent'}\n"
     ]
    }
   ],
   "source": [
    "# Create a list where train data indices are -1 and validation data indices are 0\n",
    "# X_train2 (new training set), X_train\n",
    "split_index = [-1 if x in X_train2.index else 0 for x in X.index]\n",
    "\n",
    "# Use the list to create PredefinedSplit\n",
    "pds = PredefinedSplit(test_fold = split_index)\n",
    "\n",
    "grid_search_val_LR = GridSearchCV(\n",
    "    full_pipline2, param_grid2, cv=pds, verbose=1, n_jobs=2, \n",
    "    scoring='roc_auc')\n",
    "\n",
    "# here we still use X_train; but the grid search model\n",
    "# will use our predefined split internally to determine \n",
    "# which sample belongs to the validation set\n",
    "# make fitting on the training dataset and their labels to get the best combination of the hyperparameters\n",
    "grid_search_val_LR.fit(X, y)\n",
    "\n",
    "# print the best score and the combination of the hyperparameters that got that score\n",
    "print('best score {}'.format(grid_search_val_LR.best_score_))\n",
    "print('best score {}'.format(grid_search_val_LR.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the results and saving in csv file \n",
    "submission = pd.DataFrame()\n",
    "\n",
    "# get the id from the test dataframe\n",
    "submission['id'] = ts_data.index\n",
    "\n",
    "# get the predict probability of the match value '1' by the combination of the hyperparameters that fitted the model good in the\n",
    "# training phase\n",
    "submission['match'] = grid_search_val_LR.predict_proba(df2_ts)[:,1]\n",
    "\n",
    "submission.to_csv('GS_Val_LR.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts and observations for trial 4: üòü\n",
    "This gave score on Kaggle: 0.86704 \n",
    "\n",
    "\n",
    "This model made 36 iterations in the fitting as there is 36 different combinations from the hyperparameters and I used the holdout method while fitting the model using those combinations of the hyperparameters and test them on the validation dataset to search for the best score and get the combination that gave that best score from all combinations.\n",
    "\n",
    "This model used hyperparameters as following: 'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear', 'imputer__strategy': 'most_frequent', this means that the imputer in the numerical pipeline will fill the missing values in the numerical features (columns) with the most value happen in each feature (column) in the training dataframe and test dataframe. Besides, the classifier will use solver = 'liblinear', which means that the model is using that solver as the algorithm to solve the optimization and penalty for the weights and the parameters = L2, which means the weights and the parameters of the model will be smaller. The C value is 0.01, which makes the  regularization stronger. The grid search on the holdout phase on the training dataset gave AUC score = 0.90126 and on Kaggle gave 0.86704; so this means that this combination from the hyperparameters with logistic regression model made small overfitting on the training dataset as on the unseen dataset (test dataset) this model gave a very good score but less than the XGBoost classifier, as we mentioned before, the XGBoost is better classifier than any classifier else, it is also less than the score that I got from the cross-validation method in trial 2, so this means this combination from hyperparameters didn't work well on learning on the features given.\n",
    "\n",
    "\n",
    "### Plan for trial 5: üò¢üôÜ‚Äç‚ôÄÔ∏è\n",
    "I will return to using the XGBoost classifier in the next trial. But this time I will search for the best combination of hyperparameters using Random search method which takes a random combination from the combinations of the hyperparameters at each iteration, and each time it keep the score that it gets from this iteration and after finishing all iterations which I choose its number, it gives me the combination that made the best performance through all iterations. And it calculates the score by ROCAUC score in each iteration. Using hold-out method to calculate performance using training, and validation datasets. I will use the XGBoost classifier with the same grid of the hyperparameters that I used in the first, and third trials.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Random Search with xgboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 20 candidates, totalling 20 fits\n",
      "[00:30:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "best score 0.9066683091073334\n",
      "best score {'preprocessor__num__imputer__strategy': 'most_frequent', 'my_classifier__n_estimators': 600, 'my_classifier__max_depth': 15}\n"
     ]
    }
   ],
   "source": [
    "# using the random search with XGBoost classifier\n",
    "# make 20 iteration\n",
    "random_search = RandomizedSearchCV(\n",
    "    full_pipline, param_grid, cv=pds, verbose=1, n_jobs=2, \n",
    "    # number of random trials\n",
    "    n_iter=20,\n",
    "    scoring='roc_auc')\n",
    "\n",
    "# make fitting on the training dataset and their labels to get the best combination of the hyperparameters\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# print the best score and the combination of the hyperparameters that got that score\n",
    "print('best score {}'.format(random_search.best_score_))\n",
    "print('best score {}'.format(random_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the results and saving in csv file \n",
    "submission = pd.DataFrame()\n",
    "# get the id from the test dataframe\n",
    "submission['id'] = ts_data.index\n",
    "# get the predict probability of the match value '1' by the combination of the hyperparameters that fitted the model good in the\n",
    "# training phase\n",
    "submission['match'] = random_search.predict_proba(df2_ts)[:,1]\n",
    "\n",
    "submission.to_csv('Random_Search_XGB.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts and observations for trial 5: üëÄ\n",
    "This gave score on Kaggle: 0.87600\n",
    "\n",
    "This model made 20 iterations according to the number I put. So, it will pick 20 random combinations from the combinations that were created from the grid I used in the XGBoost classifier.\n",
    "\n",
    "This model used hyperparameters as following:'max_depth': 15, 'n_estimators': 600, 'imputer__strategy': 'most_frequent', this means that the imputer in the numerical pipeline will fill the missing values in the numerical features (columns) with the most value happen in each feature (column) in the training dataframe and test dataframe. Besides, the classifier will use number of estimators = 600, which means that the model is compiled with 600 gradient boosted trees and max depth for each tree = 15, which is a little bit large number, so I thought this model made overfitting on the training dataset as the depth is large comparing with the previous times I used the same classifier. The random search on the holdout phase on the training dataset gave AUC score = 0.90666 and on Kaggle gave 0.87600, so I think there is overfitting on the training dataset as on the unseen dataset (test dataset) this model gave a very good score, but less than what I expected after I got 0.90666 score from the holdout method, and the other reason is the large number in the maximum depth. This result isn't good as the previous trials using grid search with the same classifier.\n",
    "\n",
    "\n",
    "### Plan for trial 6: üò™\n",
    "I will use the XGBoost classifier in the next trial. But this time I will search for the best combination of hyperparameters using Bayesian search method which takes a one hyperparameter value from the hyperparameters and makes calculations to get the second hyperparameter that will make a good performance, and takes the third one by some calculations until it makes a combination from the hyperparameters. It keeps doing this in searching for the hyperparameters until it finishes the number of iterations. I put. And it calculates the score by ROCAUC score in each iteration. Using holdout method to calculate performance using training, and validation datasets. I will use the hyperparameters number of estimators and max depth for each tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bayesian Search with xgboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:30:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:30:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:31:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:31:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:31:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:31:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:31:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:31:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:31:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:32:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:32:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:32:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:32:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:32:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:32:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:32:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:32:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:32:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:32:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:32:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:33:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:33:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:33:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:33:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:33:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:33:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:34:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:34:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:34:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:34:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:34:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:34:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:34:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:34:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:34:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:34:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:35:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:35:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:35:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[00:35:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:35:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "best score 0.8815566835871405\n",
      "best score OrderedDict([('my_classifier__max_depth', 5), ('my_classifier__n_estimators', 200)])\n"
     ]
    }
   ],
   "source": [
    "# create xgboost model with bayesian search\n",
    "\n",
    "XGB_pipline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('my_classifier', \n",
    "           XGBClassifier()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# define ranges for bayes search\n",
    "bayes_search = BayesSearchCV(\n",
    "    XGB_pipline,\n",
    "    {\n",
    "     # my_classifier__n_estimators points to my_classifier->n_estimators\n",
    "    'my_classifier__n_estimators': [50, 100, 200, 250, 300, 400, 500, 600],  \n",
    "     # my_classifier__max_depth points to my_classifier->max_depth\n",
    "    'my_classifier__max_depth':[5, 7, 10, 15, 17]  \n",
    "    },\n",
    "    # number of trials \n",
    "    n_iter=40,\n",
    "    random_state=0,\n",
    "    verbose=1,\n",
    "    # we still use \n",
    "    cv=pds,\n",
    ")\n",
    "\n",
    "# make fitting on the training dataset and their labels to get the best combination of the hyperparameters\n",
    "bayes_search.fit(X, y)\n",
    "\n",
    "# print the best score and the combination of the hyperparameters that got that score\n",
    "print('best score {}'.format(bayes_search.best_score_))\n",
    "print('best score {}'.format(bayes_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the results and saving in csv file \n",
    "submission = pd.DataFrame()\n",
    "# get the id from the test dataframe\n",
    "submission['id'] = ts_data.index\n",
    "\n",
    "# get the predict probability of the match value '1' by the combination of the hyperparameters that fitted the model good in the\n",
    "# training phase\n",
    "submission['match'] = bayes_search.predict_proba(df2_ts)[:,1]\n",
    "\n",
    "submission.to_csv('BS_XGB.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts and observations for trial 6: ü§î\n",
    "This gave score on Kaggle: 0.87928\n",
    "\n",
    "This model made 40 iterations according to the number I put. So, it will pick 40 random combinations from the combinations that were created from the grid I used in the XGBoost classifier.\n",
    "\n",
    "This model used hyperparameters as following:'max_depth': 5, 'n_estimators': 200, and for the numerical imputer as I didn't pass a value it will use the default value which is the 'mean', this means that the imputer in the numerical pipeline will fill the missing values in the numerical features (columns) with the mean of each feature (column) in the training dataframe and test dataframe. Besides, the classifier will use number of estimators = 200, which means that the model is compiled with 200 gradient boosted trees and max depth for each tree = 5, which is a small number, so I thought this wouldn't make overfitting on the training dataset. The bayesian search on the holdout phase on the training dataset gave AUC score = 0.88155 and on Kaggle gave 0.87928, so I think there is no overfitting on the training dataset as on the unseen dataset (test dataset) this model gave a very good score regarding this search which didn't try all combinations; it tried 40 combination only in the 40 iterations, so I think this result and those hyperparameters are very good and suitable this dataset.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall results for all trials: ü•≥\n",
    "\n",
    "\n",
    "| Trial | Score on the experimental protocol applied | Score on Kaggle |\n",
    "| :--- | :----: | ---: |\n",
    "| Grid Search with Cross-validation with xgboost classifier | 0.88106 | 0.87355 |\n",
    "| Grid Search with Cross-validation with LogisticRegression classifier | 0.85792 | 0.86746 |\n",
    "| Grid Search with validation set with xgboost classifier | 0.91110 | 0.87976 |\n",
    "| Grid Search with validation set with LogisticRegression classifier | 0.90126 | 0.86704 |\n",
    "| Random Search with xgboost classifier | 0.90666 | 0.87600 |\n",
    "| Bayesian Search with xgboost classifier | 0.88155 | 0.87928 |\n",
    "\n",
    "\n",
    "**From the above table the best trial for me was:**\n",
    "\n",
    "**Grid Search with validation set with xgboost classifier using those hyperparameters ('max_depth': 5, 'n_estimators': 100, 'imputer__strategy': 'most_frequent').**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why a simple linear regression model (without any activation function) is not good for classification task, compared to Perceptron/Logistic regression?\n",
    "\n",
    "* As linear regression gives predicted values, which are continuous, not probabilistic, in range from negative infinity to positive infinity, and we need in the classification the predicted value between 0 and 1 for the binary classification. But logistic regression gives predicted values in the range from 0 to 1.\n",
    "* The linear regression is sensitive to imbalance data, as if there is imbalance in the data, the linear regression may cause miss classification for part of them, but the logistic regression deals better with the imbalance in the data. As the linear regression tries to fit a straight line between the classes, but the logistic regression tries to fit a sigmoid curve between the classes, so the logistic regression will be better in the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's a decision tree and how it is different to a logistic regression model?\n",
    "* Decision tree is a supervised machine learning algorithm; it is used in classification and regression. It has a tree structure; its branches are the decision rules; its nodes are the features of the dataset, and its leaf nodes are the outcome.\n",
    "\n",
    "* There are a lot of differences between the decision tree and the logistic regression like: the predictive accuracy in the logistic regression is better; the logistic regression is easier to use; the logistic regression provides single coefficients for each predictor but the decision tree provides classifications rules, and they may be not numerical minded audiences. So, the logistic regression is a parametric model, but the decision tree is a non-parametric model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the difference between grid search and random search?\n",
    "* Grid search makes combinations from all hyperparameters given in the grid without repeating any combination and try all of them in the fitting of the training dataset with their labels; then it gives the best combination of the hyperparameters that will separate the two classes in the best way.\n",
    "* Random search takes a random combination from the combinations of the hyperparameters at each iteration, and each time it keep the score that it gets from that iteration, and after finishing all iterations it takes, it gives me the combination that made the best performance through all iterations.\n",
    "* Difference is grid search try all possible combinations from the hyperparameters given in the grid and get the best combination from them that gives the best score, but random search tries combinations from the hyperparameters randomly according to number of the iterations given it will try number of combinations equavilant to the number of the iterations and get the best combination from those it tried that gives the best score, so this means in the random search it doesn't be necessary that its output be the best combinations from all combinations but it is the best from the combinations it tried."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the difference between bayesian search and random search?\n",
    "\n",
    "* Random search takes a random combination from the combinations of the hyperparameters at each iteration, and each time it keep the score that it gets from that iteration, and after finishing all iterations it takes, it gives me the combination that made the best performance through all iterations.\n",
    "\n",
    "* Bayesian search takes a one hyperparameter value from the hyperparameters and makes calculations to get the second hyperparameter that will make a good performance, and takes the third one by some calculations until it makes a combination from the hyperparameters. It keeps doing this in searching for the hyperparameters until it finishes the number of iterations it takes.\n",
    "\n",
    "* So the main difference is bayesian search works on one by one hyperparameter and makes calculations to get the best combination from the hyperparameters given, and random search works by taking random combinations of hyperparameters and get the best combination that gives the best performance from the combinations it tried not from all possible combinations that may be created from the grid of the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "* Lectures and Labs\n",
    "* https://rb.gy/ky4y7f\n",
    "* https://rb.gy/yly3db\n",
    "* https://rb.gy/vlaymv\n",
    "* https://rb.gy/vzipjm\n",
    "* https://rb.gy/rxzvlk\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
