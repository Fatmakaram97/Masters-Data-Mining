{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "\n",
    "## Problem Formulation: ‚úç\n",
    "* We need to know the predicted probability of whether the new record (news title) is fake or not fake. \n",
    "\n",
    "### Input:\n",
    "* Text data for titles for news, some of them are fake and some are not. About 60000 records are labeled (train dataset) with label 1 which means fake news title, and label 0 which means not fake news title. About 59151 records are not labeled (test dataset).\n",
    "\n",
    "### Output:\n",
    "* We need to predict the probability of fake value for the given dataset (test dataset). So, this would help to know if this news title is fake (has high probability to be labeled by class 1) or not (has low probability to be labeled by class 1).\n",
    "\n",
    "\n",
    "## What data mining function is required? üïµüèΩ\n",
    "* In this assignment we need \"classification and prediction\" data mining function.\n",
    "\n",
    "## What could be the challenges? üòï\n",
    "* Challenges are: how to make data organization and deal with the missing values and news with label 2 in the train dataset. How to make good preprocessing for the text data. How to use vectorizer and make tuning for its hyperparameters and make tuning for each model used and get the best hyperparameters for the model and the vectorizer. In addition to how to develop a successful solution for the problem and make good predictions to avoid the fake titles.\n",
    "\n",
    "## What is the impact? ü§ì\n",
    "* The impact is to know more about preprocessing for the data, deal with text data, deal with vectorizer, and deal with the hyperparameters to see how they can affect the model and the results (predicted probability of whether is the label fake or not), and how to deal with the pipelines with types of searching for the hyperparameters. So, by making a good algorithm this would give the right predictions then this would help to avoid the fake news titles, so the one or the newspaper that post the news won't lose people's trust.\n",
    "\n",
    "## What is an ideal solution? ü¶∏\n",
    "* The ideal solution is to make very good learning for the model that will give perfect performance parameters, and this happens after making good preprocessing steps on the text data. So, the model can predict whether the new record (news title) is fake or not.\n",
    "\n",
    "## What is the experimental protocol used and how was it carried out? ü§î\n",
    "* I used the cross-validation method in some trials, and the holdout method in some trials. So in the cross-validation the algorithm splits the input data into training, and validation datasets, and in the holdout method I am working by splitting the given train dataset into two parts (train and test) and by selecting the test size I needed. I used the new parts for the train and test to tune the hyperparameters and get the know the best hyperparameters for the models I created by calculating the AUCROC score on that test part which I took from the given training set, so I have the right labels and by getting the predicted labels I could calculate the AUCROC score.\n",
    "\n",
    "## What preprocessing steps are used? \n",
    "* I used two preprocessing techniques:\n",
    "    * First one: get any number of white spaces with single white space - remove html tags - remove any letter not written in English language - remove any single character that has space before it and space after it - make word tokenization and split the sentences into tokens - convert any capital letter with its small letter to make all of the letters small - make stemming for all words if it is not stop word.\n",
    "    * Second one: make all letters in lowercase - Numbers removing - remove punctuation - remove white spaces - make tokenization for the words - stop words removal - make stemming for the words- make lemmatization for the words.\n",
    "    \n",
    "* As stemming means the process of removing or stems the last few characters of a word, often leading to incorrect meanings and spelling. But lemmatization means the process of consider the context and converts the word to its meaningful base form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "\n",
    "from pathlib import Path\n",
    "# some seeting for pandas\n",
    "\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 300\n",
    "pd.options.display.max_colwidth = 100\n",
    "np.set_printoptions(threshold=2000)\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import PredefinedSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "## First read the train csv file and test csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 60000 entries, 265723 to 34509\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    60000 non-null  object\n",
      " 1   label   60000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# read the train file and make the id in the csv file be the index of the dataframe\n",
    "data_train = pd.read_csv('xy_train.csv', index_col='id')\n",
    "# take a copy from the dataframe to start preprocessing technique 1\n",
    "tr_data = data_train.copy()\n",
    "# view information about the train dataset\n",
    "tr_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>265723</th>\n",
       "      <td>A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284269</th>\n",
       "      <td>British Prime Minister @Theresa_May on Nerve Attack on Former Russian Spy: \"The government has c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207715</th>\n",
       "      <td>In 1961, Goodyear released a kit that allows PS2s to be brought to heel. https://m.youtube.com/w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551106</th>\n",
       "      <td>Happy Birthday, Bob Barker! The Price Is Right Host on How He'd Like to Be Remembered | \"As the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8584</th>\n",
       "      <td>Obama to Nation: ËÅô\"Innocent Cops and Unarmed Young Black Men Should Not be Dying Before Magic Jo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70046</th>\n",
       "      <td>Finish Sniper Simo HÁõ≤yhÁõ≤ during the invasion of Finland by the USSR (1939, colorized)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189377</th>\n",
       "      <td>Nigerian Prince Scam took $110K from Kansas man; 10 years later, he's getting it back</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93486</th>\n",
       "      <td>Is It Safe To Smoke Marijuana During Pregnancy? YouÈà•Ê™á Be Surprised Of The Answer | no</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140950</th>\n",
       "      <td>Julius Caesar upon realizing that everyone in the room has a knife except him (44 bc)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34509</th>\n",
       "      <td>Jeff Bridges Releasing Èà•Ê•Ωleeping Tapes,Èà•?a New Album Designed to Help You Fall Asleep</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                       text  \\\n",
       "id                                                                                                            \n",
       "265723  A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Se...   \n",
       "284269  British Prime Minister @Theresa_May on Nerve Attack on Former Russian Spy: \"The government has c...   \n",
       "207715  In 1961, Goodyear released a kit that allows PS2s to be brought to heel. https://m.youtube.com/w...   \n",
       "551106  Happy Birthday, Bob Barker! The Price Is Right Host on How He'd Like to Be Remembered | \"As the ...   \n",
       "8584    Obama to Nation: ËÅô\"Innocent Cops and Unarmed Young Black Men Should Not be Dying Before Magic Jo...   \n",
       "...                                                                                                     ...   \n",
       "70046                 Finish Sniper Simo HÁõ≤yhÁõ≤ during the invasion of Finland by the USSR (1939, colorized)   \n",
       "189377                Nigerian Prince Scam took $110K from Kansas man; 10 years later, he's getting it back   \n",
       "93486                 Is It Safe To Smoke Marijuana During Pregnancy? YouÈà•Ê™á Be Surprised Of The Answer | no   \n",
       "140950                Julius Caesar upon realizing that everyone in the room has a knife except him (44 bc)   \n",
       "34509                 Jeff Bridges Releasing Èà•Ê•Ωleeping Tapes,Èà•?a New Album Designed to Help You Fall Asleep   \n",
       "\n",
       "        label  \n",
       "id             \n",
       "265723      0  \n",
       "284269      0  \n",
       "207715      0  \n",
       "551106      0  \n",
       "8584        0  \n",
       "...       ...  \n",
       "70046       0  \n",
       "189377      1  \n",
       "93486       0  \n",
       "140950      0  \n",
       "34509       1  \n",
       "\n",
       "[60000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display some of the training dataset\n",
    "tr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 59151 entries, 0 to 59150\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    59151 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 924.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# read the test file and make the id in the csv file be the index of the dataframe\n",
    "data_test = pd.read_csv('x_test.csv', index_col='id')\n",
    "# take a copy from the dataframe to start preprocessing technique 1\n",
    "ts_data = data_test.copy()\n",
    "# view information about the train dataset\n",
    "ts_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trials üèÉ‚Äç‚ôÄÔ∏è ü§ó\n",
    "### 1. Implement two different ways in preprocessing and apply on both Logistic Regression classifier without tuning any hyperparameters.\n",
    "### 2. A tunable pipeline including the vectorizer with word based analyzer using Logistic Regression classifier.\n",
    "### 3. Pipeline with character-level vectorizer and Logistic Regression classifier.\n",
    "### 4. Pipeline with random search with validation set and KNN classifier.\n",
    "### 5. Tuned pipeline with XGBoost classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 1:\n",
    "## Preprocessing technique number 1 with word-level-vectorizer and Logisitc regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# define our stemmer and get the stopwords from the nltk library by English language as our dataset in English language\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# define the first preprocessing function\n",
    "def preprocessing_1(text):\n",
    "    \"\"\" steps:\n",
    "        - remove any html tags (< /br> often found)\n",
    "        - Keep only ASCII + European Chars and whitespace, no digits\n",
    "        - remove single letter chars\n",
    "        - convert all whitespaces (tabs etc.) to single wspace\n",
    "        - remove stopwords, punctuation and stemm\n",
    "    \"\"\"\n",
    "    # get any number of white spaces with single white space\n",
    "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
    "    # get html tages\n",
    "    RE_TAGS = re.compile(r\"<[^>]+>\")\n",
    "    # get any letter not written in English way\n",
    "    RE_ASCII = re.compile(r\"[^A-Za-z√Ä-≈æ ]\", re.IGNORECASE)\n",
    "    # get any single character that has space before it and space after it\n",
    "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-z√Ä-≈æ]\\b\", re.IGNORECASE)\n",
    "    \n",
    "    # romove anything from the above with space and the text after what will be removed\n",
    "    text = re.sub(RE_TAGS, \" \", text)\n",
    "    text = re.sub(RE_ASCII, \" \", text)\n",
    "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
    "    text = re.sub(RE_WSPACE, \" \", text)\n",
    "    \n",
    "    # make word tokenization and split the sentences into tokens\n",
    "    word_tokens = word_tokenize(text)\n",
    "    # convert any capital letter with its small letter to make all of the letters small\n",
    "    words_tokens_lower = [word.lower() for word in word_tokens]\n",
    "\n",
    "    # stemming for all words if it is not stop word\n",
    "    words_filtered = [\n",
    "        stemmer.stem(word) for word in words_tokens_lower if word not in stop_words\n",
    "    ]\n",
    "\n",
    "    # make joining for all words in any sentence to make them one sentence again\n",
    "    text_clean = \" \".join(words_filtered)\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make cleaning for the training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Clean text in the training dataset and insert the clean text in new column\n",
    "tr_data[\"text_clean\"] = tr_data.loc[tr_data[\"text\"].str.len() > 0, \"text\"]\n",
    "tr_data[\"text_clean\"] = tr_data[\"text_clean\"].map(\n",
    "    lambda x: preprocessing_1(x) if isinstance(x, str) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Clean text in the test dataset and insert the clean text in new column\n",
    "ts_data[\"text_clean\"] = ts_data.loc[ts_data[\"text\"].str.len() > 0, \"text\"]\n",
    "ts_data[\"text_clean\"] = ts_data[\"text_clean\"].map(\n",
    "    lambda x: preprocessing_1(x) if isinstance(x, str) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the text clean column to be used in the test phase\n",
    "ts_data_clean = ts_data['text_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As there is label = 2 in the training dataset so I will change them with nan values them drop them\n",
    "tr_data.loc[tr_data[\"label\"] >1] = np.NaN\n",
    "\n",
    "\n",
    "# Drop when any of x missing\n",
    "tr_data = tr_data[(tr_data[\"text_clean\"] != \"\") & (tr_data[\"text_clean\"] != \"null\")]\n",
    "\n",
    "tr_data = tr_data.dropna(\n",
    "    axis=\"index\", subset=[\"label\", \"text\", \"text_clean\"]\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive analysis\n",
    "\n",
    "Even though we deal with texts, we should still use some descriptive analysis to get a better understanding of the data. So, I will get the words that happened the most and the words that happened the least. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "one         3285\n",
       "like        3128\n",
       "new         2998\n",
       "look        2847\n",
       "color       2737\n",
       "man         2729\n",
       "get         2602\n",
       "trump       2578\n",
       "say         2347\n",
       "peopl       2316\n",
       "use         2307\n",
       "first       2248\n",
       "make        2227\n",
       "old         2226\n",
       "time        2027\n",
       "poster      2000\n",
       "found       1999\n",
       "day         1935\n",
       "war         1858\n",
       "post        1648\n",
       "world       1570\n",
       "work        1531\n",
       "show        1513\n",
       "us          1506\n",
       "american    1504\n",
       "take        1491\n",
       "life        1482\n",
       "psbattl     1470\n",
       "help        1442\n",
       "go          1420\n",
       "state       1409\n",
       "back        1369\n",
       "two         1364\n",
       "school      1345\n",
       "see         1329\n",
       "photo       1324\n",
       "made        1314\n",
       "right       1311\n",
       "save        1308\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bokeh.models import NumeralTickFormatter\n",
    "# Word Frequency of most common words by split any sentence in the dataset and get its count in the whole documnet\n",
    "word_freq = pd.Series(\" \".join(tr_data[\"text_clean\"]).split()).value_counts()\n",
    "word_freq[1:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scoliosi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nicklaus</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>briancush</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>preprint</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>friggin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>haleakala</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>daedec</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>investitur</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>toth</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>vaynerchuck</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         index  freq\n",
       "0     scoliosi     1\n",
       "1     nicklaus     1\n",
       "2    briancush     1\n",
       "3     preprint     1\n",
       "4      friggin     1\n",
       "5    haleakala     1\n",
       "6       daedec     1\n",
       "7   investitur     1\n",
       "8         toth     1\n",
       "9  vaynerchuck     1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list most uncommon words \n",
    "word_freq[-10:].reset_index(name=\"freq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    0.538221\n",
       "1.0    0.461779\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of ratings to check whether there is unbalance in the weights or not\n",
    "tr_data[\"label\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This means there is no data unbalance as both classes have close weight to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "### Start with splitting the label and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape (59758,) (59758,)\n"
     ]
    }
   ],
   "source": [
    "# split the training data to labels and features to be used in the supervised learning :)\n",
    "# get the label column as it is our label column to be used in the supervising learning\n",
    "y = tr_data['label']\n",
    "# get the text clean column as it is the features that I will use \n",
    "X = tr_data['text_clean']\n",
    "# print the shapes of both labels and features\n",
    "print('original shape', X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make vectorization for the features with TF-IDF (Feature creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59151, 40370)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(59758, 40370)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I will start with word based analyzer without any use from other hyperparameters and keep them with the default\n",
    "vectorizer = TfidfVectorizer(analyzer=\"word\")\n",
    "# make fitting for the analyzer on the text clean column in the training dataset\n",
    "vectorizer.fit(tr_data[\"text_clean\"])\n",
    "\n",
    "# transform each sentence to numeric vector with tf-idf value as elements for the training and test datasets\n",
    "X_train_vec = vectorizer.transform(X)\n",
    "X_test_vec = vectorizer.transform(ts_data_clean)\n",
    "# display the shapes to check them\n",
    "print(X_test_vec.get_shape())\n",
    "X_train_vec.get_shape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic regression classifier\n",
    "lr1 = LogisticRegression()\n",
    "\n",
    "# make fitting for the model on the training features after make vectorization for it\n",
    "lr1.fit(X_train_vec, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving results in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the results and saving in csv file \n",
    "submission = pd.DataFrame()\n",
    "# get the id from the test dataframe\n",
    "submission['id'] = ts_data.index\n",
    "\n",
    "# get the predict probability of the label value '1' by making predict on the test features \n",
    "submission['label'] = lr1.predict_proba(X_test_vec)[:,1]\n",
    "\n",
    "submission.to_csv('trial 1 pre 1_version2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts and observations for trial 1 for preprocessing technique 1: ü§ì\n",
    "This gave score on Kaggle: 0.83466\n",
    "\n",
    "In this try, I need to deal with preprocessing more than dealing with the model. To see more how to make preprocessing and the effect of it. My expectations were that this preprocessing cover many things to be made on the text columns in the training and test datasets. Here in this try in the preprocessing, I removed whitespaces, removed any single character, removed any letter not written in English language, and removed any html tags. Then make tokenization for the words and make stemming for each word if this word is not stop word, and make all letters small. I used Logistic regression model with the default parameters for it as it is one of the best classifiers to be used in binary classification. I think this preprocessing technique got a very good results as it gave on the public leaderboard 0.83466, and I will ensure whether it is good or not after trying the second preprocessing technique.\n",
    "\n",
    "\n",
    "### Plan for trial 1 for preprocessing technique 2: ü§î\n",
    "I will change the preprocessing technique by make all letters small, remove punctuation, remove any numbers, remove extra white spaces, make tokenization for the sentences, and remove any stop word by using sklearn library. Then make stemming for the words, and make lemmatization for the words. As stemming means the process of removing or stems the last few characters of a word, often leading to incorrect meanings and spelling. But lemmatization means the process of consider the context and converts the word to its meaningful base form. I will keep using the Logistic regression classifier with its default parameters, so the change only in the preprocessing so I can compare which preprocessing technique will be more efficient and gives better results to complete my trials with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing technique number 2 with word-level-vectorizer and Logisitc regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# get the stopwords from the sklearn library by English language\n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# define our stemmer and lemmatizer by English language as our dataset in English language\n",
    "stemmer= PorterStemmer()\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "# define the second preprocessing function\n",
    "def preprocessing_2 (text):\n",
    "    # lowercase \n",
    "    input_str = text.lower()\n",
    "    \n",
    "    # Numbers removing\n",
    "    num_removed = re.sub(r'\\d+', '', input_str)\n",
    "    \n",
    "    # punctuation removing\n",
    "    pun_removed = num_removed.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # remove white spaces\n",
    "    without_space = pun_removed.strip()\n",
    "    \n",
    "    # tokenization\n",
    "    word_tokens = word_tokenize(without_space)\n",
    "    \n",
    "    # Stop words removal\n",
    "    Without_stop = [i for i in word_tokens if not i in ENGLISH_STOP_WORDS]\n",
    "    \n",
    "    \n",
    "    # stemming\n",
    "    new_word= []\n",
    "    for word in Without_stop:\n",
    "        new_word.append(stemmer.stem(word))\n",
    "    \n",
    "    # Lemmatization\n",
    "    Lemm_new_word = []\n",
    "    for word in new_word:\n",
    "        Lemm_new_word.append(lemmatizer.lemmatize(word))\n",
    "        \n",
    "   # make joining for all words in any sentence to make them one sentence again     \n",
    "    text_clean_pre2 = \" \".join(Lemm_new_word)  \n",
    "    \n",
    "    return text_clean_pre2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take copy from the training dataset to be used in the second preprocessing technique\n",
    "tr_data_newpre = data_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make cleaning for the training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 28.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Clean text in the training dataset and insert the clean text in new column using the second preprocessing technique\n",
    "tr_data_newpre[\"text_clean\"] = tr_data_newpre.loc[tr_data_newpre[\"text\"].str.len() > 0, \"text\"]\n",
    "tr_data_newpre[\"text_clean\"] = tr_data_newpre[\"text_clean\"].map(\n",
    "    lambda x: preprocessing_2(x) if isinstance(x, str) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take copy from the test dataset to be used in the second preprocessing technique\n",
    "ts_data_newpre = data_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Clean text in the test dataset and insert the clean text in new column using the second preprocessing tehnique\n",
    "ts_data_newpre[\"text_clean\"] = ts_data_newpre.loc[ts_data_newpre[\"text\"].str.len() > 0, \"text\"]\n",
    "ts_data_newpre[\"text_clean\"] = ts_data[\"text_clean\"].map(\n",
    "    lambda x: preprocessing_2(x) if isinstance(x, str) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the text clean column to be used in the test phase\n",
    "ts_data_newpre = ts_data_newpre['text_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As there is label = 2 in the training dataset so I will change them with nan values them drop them\n",
    "tr_data_newpre.loc[tr_data_newpre[\"label\"] >1] = np.NaN\n",
    "\n",
    "\n",
    "# Drop when any of x missing\n",
    "tr_data_newpre = tr_data_newpre[(tr_data_newpre[\"text_clean\"] != \"\") & (tr_data_newpre[\"text_clean\"] != \"null\")]\n",
    "\n",
    "tr_data_newpre = tr_data_newpre.dropna(\n",
    "    axis=\"index\", subset=[\"label\", \"text\", \"text_clean\"]\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "like        3074\n",
       "new         2977\n",
       "look        2825\n",
       "color       2708\n",
       "man         2592\n",
       "just        2560\n",
       "trump       2351\n",
       "say         2288\n",
       "use         2279\n",
       "peopl       2255\n",
       "make        2178\n",
       "time        1950\n",
       "poster      1936\n",
       "day         1822\n",
       "woman       1805\n",
       "war         1779\n",
       "work        1498\n",
       "help        1436\n",
       "world       1431\n",
       "life        1388\n",
       "american    1381\n",
       "old         1369\n",
       "state       1357\n",
       "photo       1310\n",
       "school      1309\n",
       "save        1289\n",
       "circa       1225\n",
       "hous        1223\n",
       "know        1222\n",
       "right       1209\n",
       "want        1207\n",
       "presid      1199\n",
       "psbattl     1164\n",
       "pictur      1160\n",
       "child       1145\n",
       "way         1132\n",
       "got         1131\n",
       "true        1122\n",
       "get         1091\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bokeh.models import NumeralTickFormatter\n",
    "# Word Frequency of most common words\n",
    "word_freq = pd.Series(\" \".join(tr_data_newpre[\"text_clean\"]).split()).value_counts()\n",
    "word_freq[1:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tchiiko</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dejuan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sunnier</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kingÁ¶Ñ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>corpÈà•Ê™ö</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>eyeand</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>beckinsal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>joal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mohn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>attackedÈà•ÊîÅnd</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          index  freq\n",
       "0       tchiiko     1\n",
       "1        dejuan     1\n",
       "2       sunnier     1\n",
       "3         kingÁ¶Ñ     1\n",
       "4        corpÈà•Ê™ö     1\n",
       "5        eyeand     1\n",
       "6     beckinsal     1\n",
       "7          joal     1\n",
       "8          mohn     1\n",
       "9  attackedÈà•ÊîÅnd     1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list most uncommon words\n",
    "word_freq[-10:].reset_index(name=\"freq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    0.538281\n",
       "1.0    0.461719\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of ratings to make sure that there is no unbalance in the dataset\n",
    "tr_data_newpre[\"label\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape (59768,) (59768,)\n"
     ]
    }
   ],
   "source": [
    "# split the training data to labels and features to be used in the supervised learning :)\n",
    "# get the match column as it is our label column\n",
    "y2 = tr_data_newpre['label']\n",
    "# drop the match column and take the rest of the dataframe to be our features\n",
    "X2 = tr_data_newpre['text_clean']\n",
    "# print the shapes of both labels and features\n",
    "print('original shape', X2.shape, y2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make vectorization for the features with TF-IDF (Feature creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59768, 50433)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use word analyzer to make the vectorization for the features of the training and features of the test\n",
    "vectorizer2 = TfidfVectorizer(analyzer=\"word\")\n",
    "vectorizer2.fit(tr_data_newpre[\"text_clean\"])\n",
    "\n",
    "# transform each sentence to numeric vector with tf-idf value as elements\n",
    "X_train_vec2 = vectorizer2.transform(X2)\n",
    "X_test_vec2 = vectorizer2.transform(ts_data_newpre)\n",
    "X_train_vec2.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fatma\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic regression classifier\n",
    "lr2 = LogisticRegression()\n",
    "# make fitting for the model on the training features after make vectorization for it\n",
    "lr2.fit(X_train_vec2, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving results in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the results and saving in csv file \n",
    "submission = pd.DataFrame()\n",
    "# get the id from the test dataframe\n",
    "submission['id'] = ts_data.index\n",
    "\n",
    "# get the predict probability of the label value '1' by making predict on the test features \n",
    "submission['label'] = lr2.predict_proba(X_test_vec2)[:,1]\n",
    "\n",
    "submission.to_csv('trial 1 pre 2_version2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts and observations for trial 1 for preprocessing technique 1: ü§ì\n",
    "This gave score on Kaggle: 0.82216\n",
    "\n",
    "In this try, I used another technique in the preprocessing by make all letters small, remove punctuation, remove any numbers, remove extra white spaces, make tokenization for the sentences, and remove any stop word. Then, make stemming for the words, and make lemmatization for the words. My expectations were to got a better results on Kaggle as I made some changes in the preprocessing, but this didn't happen. So, this means lemmatization don't make affect on the data we have this time.\n",
    "\n",
    "\n",
    "### Plan for trial 2: ü§î\n",
    "\n",
    "I will use the first technique in preprocessing, as it gave me a better results than the second one. In this trial, I will make pipeline to tune the hyperparameters for the model and the vectorizer. I will use word analyzer in the vectorizer. I will use Logistic regression model. I will tune the number of grams, max_df, and min_df in the vectorizer. I will tune C value, penalty, and solver in the classifier. As number of grams means how the vectorizer will deal with the words, whether it will make combinations from them by making 2 words together with all single words or by making 3 words together with all single words. max_df means ignore terms that appear in more than 'the number result from the tuning' of the documents. min_df  means ignore terms that appear in less than 'the number result from the tuning' of the documents. For the hyperparameters for the classifier, the solver means the algorithm that will be uses in the optimization (if the dataset is small ‚Äòliblinear‚Äô is a good choice, for multiclass problems ‚Äònewton-cg‚Äô, ‚Äòlbfgs‚Äô handle multinomial loss, and the default for the model is ‚Äòlbfgs‚Äô), the regularization means the regularization method that will be used on the weights and the parameters of the model that is used to prevent the overfitting on the training dataset, and C to control with the penality strength of the regularization as smaller values specify stronger regularization. The tuning for the hyperparameters is done by grid search method, that makes all possible combinations from the hyperparameters given in the params grid, and use them to fit the training features with their labels. Then, it gives the best hyperparameters combination that gives the best result, and makes splitting for the data in the best way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 2:\n",
    "## A tunable pipeline including the vectorizer with word based analyzer using Logistic Regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 900 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fatma\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 36min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# feature creation and modelling in a single function using pipeline\n",
    "pipe = Pipeline([(\"tfidf\", TfidfVectorizer(analyzer=\"word\")), (\"lr\", LogisticRegression())])\n",
    "\n",
    "# define parameter space to test\n",
    "params = {\n",
    "    # parameters for the vectorizer\n",
    "    \"tfidf__ngram_range\": [(1, 2),(1, 3)],\n",
    "    \"tfidf__max_df\": np.arange(0.3, 0.8),\n",
    "    \"tfidf__min_df\": np.arange(5, 30),\n",
    "    # parameters for the classifier\n",
    "    # lr__solver points to lr->solver for the classifier\n",
    "    'lr__solver' : ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "     # lr__penalty points to lr->penalty of the regularization to prevent the overfitting in the model  \n",
    "    'lr__penalty': ['l2'],\n",
    "      # lr__C points to lr->C values \n",
    "    'lr__C' : [1000, 100, 10, 1.0, 0.1, 0.01]\n",
    "}\n",
    "# n_jobs = -1 to work on all virtual processors\n",
    "# cv = 3 means 3 k folds and 3 times for cross validation by changing each time the validation part\n",
    "pipe_lr_clf = GridSearchCV(pipe, params, n_jobs=-1, scoring=\"roc_auc\",verbose=1, cv = 3)\n",
    "# make fitting for the model to try all combinations created for the hyperparameters\n",
    "pipe_lr_clf.fit(X, y)\n",
    "pickle.dump(pipe_lr_clf, open(\"./pipe_lr_clf.pck\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr__C': 1.0, 'lr__penalty': 'l2', 'lr__solver': 'lbfgs', 'tfidf__max_df': 0.3, 'tfidf__min_df': 5, 'tfidf__ngram_range': (1, 2)}\n",
      "best score 0.8621365521110835\n"
     ]
    }
   ],
   "source": [
    "# display the best combination of the hyperparameters and the best score on the training dataset using cross-validation method\n",
    "best_params = pipe_lr_clf.best_params_\n",
    "print(best_params)\n",
    "print('best score {}'.format(pipe_lr_clf.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fatma\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(max_df=0.3, min_df=5, ngram_range=(1, 2))),\n",
       "                ('lr', LogisticRegression())])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the pipe with optimized parameters\n",
    "pipe.set_params(**best_params).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the results and saving in csv file \n",
    "submission = pd.DataFrame()\n",
    "# get the id from the test dataframe\n",
    "submission['id'] = ts_data.index\n",
    "\n",
    "# get the predict probability of the label value '1' by making predict on the test features \n",
    "submission['label'] = pipe.predict_proba(ts_data_clean)[:,1]\n",
    "\n",
    "submission.to_csv('trial 2 lr_version2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts and observations for trial 2: üßê\n",
    "This gave score on Kaggle: 0.83735\n",
    "\n",
    "This model made 2700 iterations in the fitting as there are 900 different combinations from the hyperparameters and the cross-validation = 3 so the k-folds in the training and test phases will change 3 times and tried on the 900 combinations, so I got 2700 fitting iterations.\n",
    "\n",
    "This model used hyperparameters as following: 'lr__C': 1.0, 'lr__penalty': 'l2', 'lr__solver': 'lbfgs', 'tfidf__max_df': 0.3, 'tfidf__min_df': 5, 'tfidf__ngram_range': (1, 2). For the vectorizer, it used max_df = 0.3 which means ignored words that appeared in more than 30% of the documents. min_df = 5 which means ignored words that appeared in less than 5 of the documents. For the number of grams, it used (1, 2) so it made combinations from each 2 words beside using each word alone.\n",
    "Besides, the classifier will use solver = 'lbfgs', which means that the model is using the default solver as the algorithm to solve the optimization and penalty for the weights and the parameters = L2, which means the weights and the parameters of the model will be smaller. The C value is 1, which makes the regularization stronger. The grid search on the cross-validation phase on the training dataset gave AUC score = 0.86213, and on Kaggle gave 0.83735; so this means that this combination from the hyperparameters with logistic regression model didn't make overfitting on the training dataset as on the unseen dataset (test dataset) this model gave a very good score. My expectations were to get good results from this model as the Logistic regression is one of the best classifier in binary classification, and this is true as I got 0.83735 AUCROC score on Kaggle which meets my expectations.\n",
    "\n",
    "\n",
    "### Plan for trial 3: ü§®\n",
    "\n",
    "I will continue the work with the first preprocessing technique as it gave better results on Kaggle than the second technique.\n",
    "\n",
    "I will keep using the same classifier 'Logistic regression' with making tuning for the same hyperparameters that I talked about before, but I will change the analyzer in the vectorizer to character based. With keep making tuning for the rest of the hyperparameters for the vectorizer. With using the same method in searching for the hyperparameters which is grid search that I talked about in planning for trial 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 3:\n",
    "## Pipeline with character-level vectorizer and Logistic Regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 900 candidates, totalling 2700 fits\n",
      "Wall time: 1h 39min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# feature creation and modelling in a single function using pipeline\n",
    "pipe_char = Pipeline([(\"tfidf\", TfidfVectorizer(analyzer=\"char\")), (\"lr\", LogisticRegression())])\n",
    "\n",
    "# define parameter space to test \n",
    "params = {\n",
    "    # parameters for the vectorizer\n",
    "    \"tfidf__ngram_range\": [(1, 2),(1, 3)],\n",
    "    \"tfidf__max_df\": np.arange(0.3, 0.8),\n",
    "    \"tfidf__min_df\": np.arange(5, 30),\n",
    "    # parameters for the classifier\n",
    "    # lr__solver points to lr->solver for the classifier\n",
    "    'lr__solver' : ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "     # lr__penalty points to lr->penalty of the regularization to prevent the overfitting in the model  \n",
    "    'lr__penalty': ['l2'],\n",
    "      # lr__C points to lr->C values \n",
    "    'lr__C' : [1000, 100, 10, 1.0, 0.1, 0.01]\n",
    "}\n",
    "# n_jobs = -1 to work on all virtual processors\n",
    "# cv = 3 means 3 k folds and 3 times for cross validation with changing the validation part each time\n",
    "pipe_lr_clf_char = GridSearchCV(pipe_char, params, n_jobs=-1, scoring=\"roc_auc\",verbose=1, cv = 3)\n",
    "# make fitting for the model to try all combinations created for the hyperparameters\n",
    "pipe_lr_clf_char.fit(X, y)\n",
    "pickle.dump(pipe_lr_clf_char, open(\"./pipe_lr_clf.pck\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr__C': 10, 'lr__penalty': 'l2', 'lr__solver': 'liblinear', 'tfidf__max_df': 0.3, 'tfidf__min_df': 5, 'tfidf__ngram_range': (1, 3)}\n",
      "best score 0.8195522299654213\n"
     ]
    }
   ],
   "source": [
    "# display the best combination of the hyperparameters and the best score on the training dataset using cross-validation method\n",
    "best_params = pipe_lr_clf_char.best_params_\n",
    "print(best_params)\n",
    "print('best score {}'.format(pipe_lr_clf_char.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='char', max_df=0.3, min_df=5,\n",
       "                                 ngram_range=(1, 3))),\n",
       "                ('lr', LogisticRegression(C=10, solver='liblinear'))])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run pipe with optimized parameters\n",
    "pipe_char.set_params(**best_params).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the results and saving in csv file \n",
    "submission = pd.DataFrame()\n",
    "# get the id from the test dataframe\n",
    "submission['id'] = ts_data.index\n",
    "\n",
    "# get the predict probability of the label value '1' by making predict on the test features \n",
    "submission['label'] = pipe_char.predict_proba(ts_data_clean)[:,1]\n",
    "\n",
    "submission.to_csv('trial 3 lr char_version2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts and observations for trial 3: ‚úå\n",
    "This gave score on Kaggle: 0.77437\n",
    "\n",
    "This model made 2700 iterations in the fitting as there are 900 different combinations from the hyperparameters and the cross-validation = 3 so the k-folds in the training and test phases will change 3 times and tried on the 900 combinations, so I got 2700 fitting iterations.\n",
    "\n",
    "This model used hyperparameters as following:'lr__C': 10, 'lr__penalty': 'l2', 'lr__solver': 'liblinear', 'tfidf__max_df': 0.3, 'tfidf__min_df': 5, 'tfidf__ngram_range': (1, 3). So, the model used the solver = 'liblinear', which means that the model is using that solver as the algorithm to solve the optimization and penalty for the weights and the parameters = L2, which means the weights and the parameters of the model will be smaller. The C value is 10, which makes the regularization stronger, but it is a little bit big number, so the regularization isn't too strong on the weights and parameters for the model. For the vectorizer, it used max_df = 0.3 which means ignore terms that appear in more than 0.3 of the documents. min_df = 5 which means ignore terms that appear in less than 5 of the documents. For the number of grams, it used (1, 3), so it made combinations from each 3 words beside using each word alone. \n",
    "My expectations were to got better results as I used character based in the vectorizer, and it is better than the word based. But I got score = 0.81955 from fitting on the training dataset, and I got score = 0.77437 from Kaggle which isn't my expectation from this trial.\n",
    "\n",
    "\n",
    "### Plan for trial 4: ü§ï\n",
    "\n",
    "I will continue the work with the first preprocessing technique as it gave better results on Kaggle than the second technique.\n",
    "\n",
    "I will change the classifier in the next trial and I will use K nearest neighbor classifier with continue using holdout way by splitting the training dataset into training and validation datasets. Adjusting the hyperparameters: weights, number of neighbors, and metric using the random search method, which takes a random combination from the combinations of the hyperparameters at each iteration, and each time it keeps the score that it gets from this iteration and after finishing all iterations which I choose its number, it gives me the combination that made the best performance through all iterations. And it calculates the score by ROCAUC score in each iteration. About the hyperparameters, the weights which means weight function used in prediction, metric which means the distance metric to use for the tree, and number of neighbors which means the number neighbors that used in the calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 4:\n",
    "## Pipeline with random search with validation set and KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training features and labels into training and validation dataset with stratify the label so each part will contain\n",
    "# suitable number of each label \n",
    "X_train2, X_val, y_train2, y_val = train_test_split(\n",
    "    X, y, train_size = 0.8, stratify = y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 40 candidates, totalling 40 fits\n",
      "best score 0.8320932616454605\n",
      "best score {'tfidf__min_df': 7, 'tfidf__max_df': 0.3, 'KNN__weights': 'distance', 'KNN__n_neighbors': 29, 'KNN__metric': 'minkowski'}\n",
      "Wall time: 6min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create a list where train data indices are -1 and validation data indices are 0\n",
    "# X_train2 (new training set), X the main training set\n",
    "split_index = [-1 if x in X_train2.index else 0 for x in X.index]\n",
    "\n",
    "# Use the list to create PredefinedSplit\n",
    "pds = PredefinedSplit(test_fold = split_index)\n",
    "\n",
    "# define parameter space to test \n",
    "param_random = {\n",
    "    # parameters for the vectorizer\n",
    "    \"tfidf__max_df\": np.arange(0.3, 0.8),\n",
    "    \"tfidf__min_df\": np.arange(5, 30),\n",
    "    # parameters for the classifier\n",
    "    # KNN__metric points to KNN->metric for the classifier\n",
    "    'KNN__metric' : ['euclidean', 'manhattan', 'minkowski'],\n",
    "    # KNN__weights points to KNN->weights of the regularization to prevent the overfitting in the model  \n",
    "    'KNN__weights': ['uniform', 'distance'],\n",
    "      # KNN__n_neighbors points to KNNr->n_neighbors\n",
    "    'KNN__n_neighbors' : range(1, 31, 2)\n",
    "}\n",
    "\n",
    "# feature creation and modelling in a single function using pipeline\n",
    "pipe_word_KNN = Pipeline([(\"tfidf\", TfidfVectorizer(analyzer=\"word\")), (\"KNN\", KNeighborsClassifier())])\n",
    "\n",
    "# use random search method to search for the hyperparameters with using the predefined split in the cv to use the validation set\n",
    "random_search_val_KNN = RandomizedSearchCV(\n",
    "    pipe_word_KNN, param_random, cv=pds, verbose=1, n_jobs=2, \n",
    "    # number of trials \n",
    "    n_iter=40,\n",
    "    scoring='roc_auc')\n",
    "\n",
    "\n",
    "# will use our predefined split internally to determine \n",
    "# which sample belongs to the validation set\n",
    "# make fitting on the training dataset and their labels to get the best combination of the hyperparameters\n",
    "random_search_val_KNN.fit(X, y)\n",
    "\n",
    "# print the best score and the combination of the hyperparameters that got that score\n",
    "print('best score {}'.format(random_search_val_KNN.best_score_))\n",
    "print('best score {}'.format(random_search_val_KNN.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tfidf__min_df': 7, 'tfidf__max_df': 0.3, 'KNN__weights': 'distance', 'KNN__n_neighbors': 29, 'KNN__metric': 'minkowski'}\n",
      "best score 0.8320932616454605\n"
     ]
    }
   ],
   "source": [
    "# display the best combination of the hyperparameters and the best score on the training dataset using cross-validation method\n",
    "best_params = random_search_val_KNN.best_params_\n",
    "print(best_params)\n",
    "print('best score {}'.format(random_search_val_KNN.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer(max_df=0.3, min_df=7)),\n",
       "                ('KNN',\n",
       "                 KNeighborsClassifier(n_neighbors=29, weights='distance'))])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run pipe with optimized parameters\n",
    "pipe_word_KNN.set_params(**best_params).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the results and saving in csv file \n",
    "submission = pd.DataFrame()\n",
    "# get the id from the test dataframe\n",
    "submission['id'] = ts_data.index\n",
    "\n",
    "# get the predict probability of the label value '1' by making predict on the test features \n",
    "submission['label'] = pipe_word_KNN.predict_proba(ts_data_clean)[:,1]\n",
    "\n",
    "submission.to_csv('trial 4 knn bayes_version2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts and observations for trial 4: üëÄ\n",
    "This gave score on Kaggle: 0.77379\n",
    "\n",
    "This model made 40 iterations according to the number I put. So, it will pick 40 random combinations from the combinations that were created from the grid I made in this trial.\n",
    "\n",
    "This model used hyperparameters as following:'tfidf__min_df': 7, 'tfidf__max_df': 0.3, 'KNN__weights': 'distance', 'KNN__n_neighbors': 29, 'KNN__metric': 'minkowski', this means that in the vectorizer, it used min_df = 7, so it ignored any word appeared less than 7 times in the documents. max_df = 0.3, so it ignored any word appeared in 30% of the documents. It used the default n-grams which equal (1, 1), so it used words and didn't make any combinations from the words. For the classifier, it used number of neighbors that are used in calculation of the distance = 29. It used the weights = 'distance', so in this case, closer neighbors of a query point had a greater influence than neighbors which are further away. It used metric = 'minkowski' which means it calculated the distance in a normed vector space, which means in a space where distances can be represented as a vector that has a length and the lengths cannot be negative according to the following equation: \n",
    "![alt text](https://www.kdnuggets.com/wp-content/uploads/popular-knn-metrics-1.png \"minkowski equation\").\n",
    "\n",
    "My expectations were to have a good result, as from the calculated score on the training phase, I got score = 0.83209. On Kaggle I got 0.77379 although I used high number of iterations to get the best combination of the hyperparameters, so this model didn't work well on the unseen dataset (test data).\n",
    "\n",
    "\n",
    "### Plan for trial 5: üò™\n",
    "I will use the XGBoost classifier in the next trial with cross-validation as the experimental protocol. For the hyperparameters that I will use with this classifier: the number of the estimators, which refers to the number of gradient boosted trees that will be used in the model, I will pass different numbers for it from small number to big number to see what the best number of estimators is, the max depth, which refers to the maximum tree depth for the base learners, I will use a little bit large numbers as we have a very large dataset, and not too big, so this won't make overfitting in the training phase. For the vectorizer I will search for the best max_df and min_df.\n",
    "\n",
    "But this time I will search for the best combination of hyperparameters using Bayesian search method which takes a one hyperparameter value from the hyperparameters and makes calculations to get the second hyperparameter that will make a good performance, and takes the third one by some calculations until it makes a combination from the hyperparameters. It keeps doing this in searching for the hyperparameters until it finishes the number of iterations that I give to the model. And it calculates the score by ROCAUC score in each iteration. Using holdout method to calculate performance using training, and validation datasets by the cross-validation method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 5:\n",
    "## Tuned pipeline with XGBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fatma\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fatma\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fatma\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fatma\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:12:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Wall time: 2h 39min 53s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=2,\n",
       "              estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                        ('xgb',\n",
       "                                         XGBClassifier(base_score=None,\n",
       "                                                       booster=None,\n",
       "                                                       colsample_bylevel=None,\n",
       "                                                       colsample_bynode=None,\n",
       "                                                       colsample_bytree=None,\n",
       "                                                       enable_categorical=False,\n",
       "                                                       gamma=None, gpu_id=None,\n",
       "                                                       importance_type=None,\n",
       "                                                       interaction_constraints=None,\n",
       "                                                       learning_rate=None,\n",
       "                                                       max_delta_step=None,\n",
       "                                                       max_depth=None,\n",
       "                                                       min_chil...\n",
       "                                                       scale_pos_weight=None,\n",
       "                                                       subsample=None,\n",
       "                                                       tree_method=None,\n",
       "                                                       validate_parameters=None,\n",
       "                                                       verbosity=None))]),\n",
       "              n_iter=60, n_jobs=-1, scoring='roc_auc',\n",
       "              search_spaces={'tfidf__max_df': array([0.3]),\n",
       "                             'tfidf__min_df': array([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
       "       22, 23, 24, 25, 26, 27, 28, 29]),\n",
       "                             'xgb__max_depth': [50, 100, 200, 300],\n",
       "                             'xgb__n_estimators': [80, 100, 200, 300, 400]},\n",
       "              verbose=1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# feature creation with word based analyzer and modelling in a single function using pipeline\n",
    "pipe_xgb = Pipeline([(\"tfidf\", TfidfVectorizer(analyzer=\"word\")), (\"xgb\", XGBClassifier())])\n",
    "\n",
    "# define parameter space to test\n",
    "params_xgb = {\n",
    "    # parameters for the vectorizer\n",
    "    \"tfidf__max_df\": np.arange(0.3, 0.8),\n",
    "    \"tfidf__min_df\": np.arange(5, 30),\n",
    "    # parameters for the classifier\n",
    "    # xgb__n_estimators points to xgb->number of estimators\n",
    "    'xgb__n_estimators': [80, 100, 200, 300, 400],  \n",
    "     # xgb__max_depth points to xgb->max_depth\n",
    "    'xgb__max_depth':[50, 100, 200, 300]  \n",
    "}\n",
    "\n",
    "# use bayes search to find the best combination of the hyperparameters with cross-validation = 2 which means 2 k folds and 2 \n",
    "# times to change the validation set each time\n",
    "pipe_xgb_clf = BayesSearchCV(pipe_xgb, params_xgb, n_jobs=-1, cv=2, scoring=\"roc_auc\", verbose = 1,\n",
    "     # number of trials \n",
    "        n_iter=60)\n",
    "# make fitting with each combination made to find the best one that gives the best results\n",
    "pipe_xgb_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('tfidf__max_df', 0.3), ('tfidf__min_df', 10), ('xgb__max_depth', 50), ('xgb__n_estimators', 200)])\n",
      "best score 0.8171171162030598\n"
     ]
    }
   ],
   "source": [
    "# display the best combination of the hyperparameters and the best score on the training dataset using cross-validation method\n",
    "best_params = pipe_xgb_clf.best_params_\n",
    "print(best_params)\n",
    "print('best score {}'.format(pipe_xgb_clf.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fatma\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:14:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5455636 , 0.45443645],\n",
       "       [0.5455636 , 0.45443645],\n",
       "       [0.29311693, 0.7068831 ],\n",
       "       ...,\n",
       "       [0.99691194, 0.00308808],\n",
       "       [0.89561254, 0.10438748],\n",
       "       [0.08564711, 0.9143529 ]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run pipe with optimized parameters\n",
    "pipe_xgb.set_params(**best_params).fit(X, y)\n",
    "pipe_pred = pipe_xgb.predict_proba(ts_data_clean)\n",
    "pipe_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the results and saving in csv file \n",
    "submission = pd.DataFrame()\n",
    "# get the id from the test dataframe\n",
    "submission['id'] = ts_data.index\n",
    "\n",
    "# get the predict probability of the label value '1' by making predict on the test features \n",
    "submission['label'] = pipe_xgb.predict_proba(ts_data_clean)[:,1]\n",
    "\n",
    "submission.to_csv('trial 5 xgb_version2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts and observations for trial 5: ü§î\n",
    "This gave score on Kaggle: 0.79731\n",
    "\n",
    "\n",
    "\n",
    "This model made 60 iterations according to the number I set. So, it will pick 60 combinations by getting the combination of the hyperparameters by the way I mentioned in the planning for this trial.\n",
    "\n",
    "\n",
    "\n",
    "This model used hyperparameters as following: ('tfidf__max_df', 0.3), ('tfidf__min_df', 10), ('xgb__max_depth', 50), ('xgb__n_estimators', 200). First, for the vectorizer, it used the default n-grams which equal (1, 1), so it used words and didn't make any combinations of the words. max_df = 0.3, meaning it ignored any word that appeared in the documents more than 30%. min_df = 10 means it ignored any word appeared less than 10 times in the documnets.\n",
    "\n",
    "Besides, the classifier will use a number of estimators = 200, which means that the model is compiled with 200 gradient boosted trees and max depth for each tree = 50, which is the smallest number, so I thought this wouldn't be overfit on the training dataset. The bayesian search on the cross-validation phase on the training dataset gave AUC score = 0.81711 and on Kaggle gave 0.79731, so I think there is no overfitting on the training dataset as on the unseen dataset (test dataset). This model gave a good score which is close to the one I got from trying on the training dataset regarding this search, which didn't try all combinations; it tried 60 combination only in the 60 iterations, so I think this result and those hyperparameters are very good and suitable for this  dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall results for all trials: ü•≥\n",
    "\n",
    "\n",
    "| Trial | Score on the experimental protocol applied | Score on Kaggle |\n",
    "| :--- | :----: | ---: |\n",
    "| Try with the first technique in preprocessing | No experimental protocol | 0.83466 |\n",
    "| Try with the second technique in preprocessing | No experimental protocol | 0.82216 |\n",
    "| A tunable pipeline including the vectorizer with word based analyzer using Logistic Regression classifier. | 0.86213 | 0.83735 |\n",
    "| Pipeline with character-level vectorizer and Logistic Regression classifier | 0.81955 | 0.77437 |\n",
    "| Pipeline with random search with validation set and KNN classifier | 0.83209 | 0.77379 |\n",
    "| Tuned pipeline with XGBoost classifier | 0.81711 | 0.79731 |\n",
    "\n",
    "\n",
    "\n",
    "**From the above table the best trial for me was:**\n",
    "\n",
    "**A tunable pipeline including the vectorizer with word based analyzer using Logistic Regression classifier with using those hyperparameters 'lr__C': 1.0, 'lr__penalty': 'l2', 'lr__solver': 'lbfgs', 'tfidf__max_df': 0.3, 'tfidf__min_df': 5, 'tfidf__ngram_range': (1, 2).**\n",
    "**Besides, the TF-IDF vectorization with word analyzer gave me better results than the character analyzer, so my preprocessing steps and implementation for the model didn't work well with the character analyzer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the difference between Character n-gram and Word n-gram? Which one tends to suffer more from the OOV issue?\n",
    "\n",
    "* Character n-gram splits the sentence according to n number of characters, like if we have sentences: 'I play volleyball' and n = 3, so it will split into the following: I p - pla - lay - vol - oll - .... and so on.\n",
    "\n",
    "* Word n-gram splits the sentence according to n number of words, like if we have sentences: 'I play volleyball' and n = 2, so it will split into the following: I play - play volleyball.\n",
    "\n",
    "* The one that tends to suffer more from the Out Of Vocabulary (OOV) is: Word n-gram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the difference between stop word removal and stemming? Are these techniques language-dependent?\n",
    "\n",
    "* Stop word removal means removing unimportant words that may happen too much in the data without importance in the model, so this reduces the dataset size and thus reduces the training time due to the lower number of tokens involved in the training. Like removing am - is - are - .... and so on in English.\n",
    "\n",
    "* Stemming means removing morphological affixes from words, leaving only the word stem. Like in word different, it become differ.\n",
    "\n",
    "* Yes, they are language-dependent, as both change from one language to another. For example, 'is' is a stop word in English, and 'ist; is a stop word in German."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is tokenization techniques language dependent? Why?\n",
    "* Yes, tokenization is language dependent.\n",
    "\n",
    "* As each language has its own shape and letters, so in some languages there are white spaces between words and some don't have white spaces. And some languages combine words together with shapes related to this language, so tokenization may not split the combination of two words and take it as one token instead of splitting them and taking them as two tokens. For example, languages like Vietnamese and Chinese, if we used English tokenization with them, it wouldn't work well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the difference between count vectorizer and tf-idf vectorizer? Would it be feasible to use all possible n-grams? If not, how should you select them?\n",
    "\n",
    "* Count vectorizer means counting how many times the word appears in the document. So, here it considers the count of each unique word in the document.\n",
    "\n",
    "* TF-IDF vectorizer means getting the weight of word counts by measuring how often the word appears in the documents. So, here it considers the overall document weightage of a word.\n",
    "\n",
    "* It wouldn't be feasible to use all possible n-grams.\n",
    "\n",
    "* As to select the number of grams, so it depends on the dataset and the application. In some applications, it is important to deal with some combinations of words together as they are together, have a big effect. For example: if we talk about a dataset for rating a company, and the sentence 'company good organized' is important for us, so we need to make the number of grams = 3 to capture this combination of words, so here we use trigrams, and this would have a big effect in increasing the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "* https://rb.gy/rvvfmq\n",
    "* https://rb.gy/pv963r\n",
    "* https://rb.gy/ylzjdd\n",
    "* https://rb.gy/imeowk\n",
    "* https://rb.gy/9xua2n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
